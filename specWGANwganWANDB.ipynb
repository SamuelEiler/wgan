{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1specNormGANwganWANDB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1373f14fe26b4607a4a52fce3e3348ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a1983f76cad8490986ecfa5045fe63de",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_be8b48eae1cf4b368fb48890850618d8",
              "IPY_MODEL_fe0572276b6a442ea2f1510cd044d2f8"
            ]
          }
        },
        "a1983f76cad8490986ecfa5045fe63de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "be8b48eae1cf4b368fb48890850618d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_fc84bd6209b8474e9b8c38cdee8c57cb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.11MB of 1.11MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_57ef33b862fe484e9f9c5329c36a1597"
          }
        },
        "fe0572276b6a442ea2f1510cd044d2f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_72de9774c3af490999c7d34bec8cfe71",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_561cee3f8a0b4b699e0f7e5e30753c76"
          }
        },
        "fc84bd6209b8474e9b8c38cdee8c57cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "57ef33b862fe484e9f9c5329c36a1597": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "72de9774c3af490999c7d34bec8cfe71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "561cee3f8a0b4b699e0f7e5e30753c76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "31473c81c5fb4ff9b304d7a7cf8589a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c61d53f6c26d439bbc3defc2808ed879",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ec026d8279634565b4c2e2a14a95040e",
              "IPY_MODEL_3e83d2b996c548c9b0785038803ea6c7"
            ]
          }
        },
        "c61d53f6c26d439bbc3defc2808ed879": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ec026d8279634565b4c2e2a14a95040e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d0ca1ef722f84350b95f176ff80d98c4",
            "_dom_classes": [],
            "description": " 62%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 250,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 155,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_97c959d4148743d38c0fb8e5c5095481"
          }
        },
        "3e83d2b996c548c9b0785038803ea6c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b03acd29928d4c90acddd9b51184f641",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 155/250 [25:58&lt;16:02, 10.13s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c5cc79d68e3947519db8e5ba5638694b"
          }
        },
        "d0ca1ef722f84350b95f176ff80d98c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "97c959d4148743d38c0fb8e5c5095481": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b03acd29928d4c90acddd9b51184f641": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c5cc79d68e3947519db8e5ba5638694b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamuelEiler/wgan/blob/main/specWGANwganWANDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdDb6jqqu31M"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFQp8W5VWaJu",
        "outputId": "4c9520ed-b2e4-4418-f293-2224b22026d6"
      },
      "source": [
        "!pip install wandb -qqq\r\n",
        "!pip install --upgrade wandb\r\n",
        "!wandb login 50457c76bb245c5d932008de50c1858da49aeeba\r\n",
        "import wandb\r\n",
        "#wandb.init(project=\"sWGAN\")\r\n",
        "\r\n",
        "from __future__ import division\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from torchvision import transforms\r\n",
        "from torch.autograd.variable import Variable\r\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
        "\r\n",
        "import plotly.figure_factory as ff\r\n",
        "import torch\r\n",
        "import numpy as np\r\n",
        "import math\r\n",
        "\r\n",
        "\r\n",
        "import seaborn as sns\r\n",
        "import plotly.express as px\r\n",
        "from torch import autograd\r\n",
        "from torch.distributions.normal import Normal\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch import nn\r\n",
        "from torch import Tensor\r\n",
        "from torch.nn import Parameter\r\n",
        "\r\n",
        "sns.set(rc={'figure.figsize':(11, 8)})\r\n",
        "\r\n",
        "import datetime \r\n",
        "from datetime import date\r\n",
        "today = date.today()\r\n",
        "\r\n",
        "import random\r\n",
        "import json as js\r\n",
        "import pickle\r\n",
        "import os\r\n",
        "import tqdm.notebook as tq\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import random\r\n",
        "import plotly.graph_objects as go\r\n",
        "\r\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "random_seed = 29\r\n",
        "\r\n",
        "np.random.seed(random_seed)\r\n",
        "torch.manual_seed(random_seed)\r\n",
        "torch.set_deterministic(True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: wandb in /usr/local/lib/python3.6/dist-packages (0.10.13)\n",
            "Requirement already satisfied, skipping upgrade: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied, skipping upgrade: watchdog<0.10.5,>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.10.4)\n",
            "Requirement already satisfied, skipping upgrade: shortuuid>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: configparser>=3.8.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.0.1)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: docker-pycreds>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied, skipping upgrade: subprocess32>=3.5.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: GitPython>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.1.12)\n",
            "Requirement already satisfied, skipping upgrade: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied, skipping upgrade: sentry-sdk>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.19.5)\n",
            "Requirement already satisfied, skipping upgrade: pathtools>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from watchdog<0.10.5,>=0.8.3->wandb) (0.1.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.12.0->wandb) (51.1.1)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from GitPython>=1.0.0->wandb) (4.0.5)\n",
            "Requirement already satisfied, skipping upgrade: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.4)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694,
          "referenced_widgets": [
            "1373f14fe26b4607a4a52fce3e3348ab",
            "a1983f76cad8490986ecfa5045fe63de",
            "be8b48eae1cf4b368fb48890850618d8",
            "fe0572276b6a442ea2f1510cd044d2f8",
            "fc84bd6209b8474e9b8c38cdee8c57cb",
            "57ef33b862fe484e9f9c5329c36a1597",
            "72de9774c3af490999c7d34bec8cfe71",
            "561cee3f8a0b4b699e0f7e5e30753c76"
          ]
        },
        "id": "Zx6Y53jyu8LZ",
        "outputId": "f8eeeac1-d030-4174-c3a8-0c7deb1a0205"
      },
      "source": [
        "\"\"\"Model loading/saving\"\"\"\r\n",
        "dataPath = './dataset'  \r\n",
        "\r\n",
        "\r\n",
        "trainCounter = 0\r\n",
        "loadVersion = 0\r\n",
        "loadModel = False\r\n",
        "epoch_loaded = 0\r\n",
        "path = './SineGAN/' + str(trainCounter)\r\n",
        "\r\n",
        "loadPath = '/content/epoch500.tar'\r\n",
        "\r\n",
        "\r\n",
        "#loadPath = '/content/SineGAN/0/epoch148.tar'\r\n",
        "\r\n",
        "\r\n",
        "\"\"\"Defining parameters\"\"\"\r\n",
        "noise_features = 3 #frequency, amplitude, shift\r\n",
        "sample_size = 64 * 2 #batch size needed for Data Loader and the noise creator function.\r\n",
        "\r\n",
        "\r\n",
        "#Params for the generator\r\n",
        "hidden_nodes_g = 50\r\n",
        "layers = 1\r\n",
        "tanh_layer = False\r\n",
        "bidir = False\r\n",
        "\r\n",
        "dropout = 0.05\r\n",
        "\r\n",
        "#No. of training rounds per epoch\r\n",
        "D_rounds = 1\r\n",
        "G_rounds = 1\r\n",
        "num_epoch = 250\r\n",
        "#learning_rate = 0.0001 # 0.00005  * 200 #*2 for adam \r\n",
        "lr_decrease_factor = 0.98\r\n",
        "lr_start_factor = 10\r\n",
        "lr_g = 0.00001 #* 0.5\r\n",
        "lr_d = 0.001 #* 0.5\r\n",
        "beta1 = 0.0\r\n",
        "beta2 = 0.999\r\n",
        "opt_adam = True #working: adam lr*2 lp3\r\n",
        "opt = ''\r\n",
        "if opt_adam:\r\n",
        "  opt = 'Adam'\r\n",
        "else:\r\n",
        "  opt = 'RMS'\r\n",
        "#learning_rate=0.0002, beta_1=0.5, beta_2=0.9\r\n",
        "\r\n",
        "lipschitz_constraint = 10\r\n",
        "center = 1\r\n",
        "onesided = True\r\n",
        "penalty_weight = 10\r\n",
        "if onesided:\r\n",
        "    clip_fn = lambda x: x.clamp(max=0)\r\n",
        "else:\r\n",
        "    clip_fn = lambda x: x    \r\n",
        "#Params for the Discriminator\r\n",
        "minibatch_layer = 0\r\n",
        "minibatch_normal_init_ = True\r\n",
        "num_cvs = 2\r\n",
        "cv1_out= 5\r\n",
        "cv1_k = 3\r\n",
        "cv1_s = 2\r\n",
        "p1_k = 3\r\n",
        "p1_s = 2\r\n",
        "cv2_out = 10\r\n",
        "cv2_k = 5\r\n",
        "cv2_s = 2\r\n",
        "p2_k = 3\r\n",
        "p2_s = 2\r\n",
        "\r\n",
        "wandb.init(project=\"sWGAN\", config={\r\n",
        "    'center_gp': center,\r\n",
        "    'dropout': dropout,\r\n",
        "    'random_seed': random_seed,\r\n",
        "    \"lipschitz_constraint\": lipschitz_constraint,\r\n",
        "    \"D_rounds\": D_rounds,\r\n",
        "    \"optim\": opt,\r\n",
        "    'beta1': beta1,\r\n",
        "    'beta2': beta2,\r\n",
        "    'penalty_weight': penalty_weight,\r\n",
        "    'one_side': onesided,\r\n",
        "    'sample_size' : sample_size, \r\n",
        "    'noise_features' : noise_features,\r\n",
        "    'seq_length' : 50,\r\n",
        "    'num_layers': layers, \r\n",
        "    'bidir': bidir,\r\n",
        "    'hidden-initialization': 'zeros',\r\n",
        "    'hidden_dims_generator': hidden_nodes_g, \r\n",
        "    'minibatch_layer': minibatch_layer,\r\n",
        "    'minibatch_normal_init_' : minibatch_normal_init_,\r\n",
        "    'num_cvs':num_cvs,\r\n",
        "    'cv1_out':cv1_out,\r\n",
        "    'cv1_k':cv1_k,\r\n",
        "    'cv1_s':cv1_s,\r\n",
        "    'p1_k':p1_k,\r\n",
        "    'p1_s':p1_s,\r\n",
        "    'cv2_out':cv2_out,\r\n",
        "    'cv2_k':cv2_k,\r\n",
        "    'cv2_s':cv2_s,\r\n",
        "    'p2_k':p2_k,\r\n",
        "    'p2_s':p2_s,\r\n",
        "    'num_epoch':num_epoch,\r\n",
        "    'D_rounds': D_rounds,\r\n",
        "    'G_rounds': G_rounds,  \r\n",
        "    'D_lr' : lr_d,\r\n",
        "    'G_lr' : lr_g,\r\n",
        "    'lr_decrease_factor': lr_decrease_factor,\r\n",
        "    'lr_start_factor': lr_start_factor\r\n",
        "\r\n",
        "\r\n",
        "})\r\n",
        "config = wandb.config\r\n",
        "\r\n",
        "\r\n",
        "lr_g = lr_g * lr_start_factor\r\n",
        "lr_d = lr_d * lr_start_factor"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:3m890twc) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 221<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1373f14fe26b4607a4a52fce3e3348ab",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 1.02MB of 1.02MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210114_113111-3m890twc/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210114_113111-3m890twc/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>Critic</td><td>-1.84763</td></tr><tr><td>Generator</td><td>3.98805</td></tr><tr><td>GradientPenalty</td><td>0.21467</td></tr><tr><td>_step</td><td>10</td></tr><tr><td>_runtime</td><td>67</td></tr><tr><td>_timestamp</td><td>1610623941</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>Critic</td><td>█▇▁</td></tr><tr><td>Generator</td><td>█▃▁</td></tr><tr><td>GradientPenalty</td><td>▂█▁</td></tr><tr><td>_step</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>_runtime</td><td>▁▁▁▁▃▃▃▅▅▅█</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▃▃▃▅▅▅█</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 9 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">earthy-planet-17</strong>: <a href=\"https://wandb.ai/samueleiler/sWGAN/runs/3m890twc\" target=\"_blank\">https://wandb.ai/samueleiler/sWGAN/runs/3m890twc</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "...Successfully finished last run (ID:3m890twc). Initializing new run:<br/><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.13<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">feasible-cherry-18</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/samueleiler/sWGAN\" target=\"_blank\">https://wandb.ai/samueleiler/sWGAN</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/samueleiler/sWGAN/runs/26ibxya9\" target=\"_blank\">https://wandb.ai/samueleiler/sWGAN/runs/26ibxya9</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210114_113221-26ibxya9</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9HeajX2fbH-"
      },
      "source": [
        "ecg = 0\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "if ecg:\r\n",
        "\r\n",
        "  from google.colab import drive\r\n",
        "  drive.mount('/content/drive')\r\n",
        "\r\n",
        "  !pip install arff2pandas\r\n",
        "\r\n",
        "  from arff2pandas import a2p\r\n",
        "\r\n",
        "  path = '/content/drive/MyDrive/MasterThesis/ECG_Dataset/ECG5000/'\r\n",
        "  with open(path+'ECG5000_TRAIN.arff') as f:\r\n",
        "    train = a2p.load(f)\r\n",
        "  with open(path+'ECG5000_TEST.arff') as f:\r\n",
        "    test = a2p.load(f)\r\n",
        "\r\n",
        "  df = train.append(test)\r\n",
        "  df = df.sample(frac=1.0)\r\n",
        "  df.shape\r\n",
        "\r\n",
        "  CLASS_NORMAL = 1\r\n",
        "  class_names = ['Normal','R on T','PVC','SP','UB']\r\n",
        "\r\n",
        "  new_columns = list(df.columns)\r\n",
        "  new_columns[-1] = 'target'\r\n",
        "  df.columns = new_columns\r\n",
        "\r\n",
        "  df.target.value_counts()\r\n",
        "\r\n",
        "  normal_df = df[df.target == str(CLASS_NORMAL)].drop(labels='target', axis=1)\r\n",
        "  print(normal_df.shape)\r\n",
        "\r\n",
        "  anomaly_df = df[df.target != str(CLASS_NORMAL)].drop(labels='target', axis=1)\r\n",
        "  print(anomaly_df.shape)\r\n",
        "\r\n",
        "  from sklearn.model_selection import train_test_split\r\n",
        "  RANDOM_SEED = 29\r\n",
        "  np.random.seed(RANDOM_SEED)\r\n",
        "  torch.manual_seed(RANDOM_SEED)\r\n",
        "\r\n",
        "\r\n",
        "  train_df, val_df = train_test_split(\r\n",
        "    normal_df,\r\n",
        "    test_size=0.15,\r\n",
        "    random_state=RANDOM_SEED\r\n",
        "  )\r\n",
        "\r\n",
        "  data = pd.DataFrame(train_df[1:][:])  \r\n",
        "\r\n",
        "  dataPath = './dataset'  \r\n",
        "  if not os.path.exists(dataPath):\r\n",
        "    os.makedirs(dataPath)\r\n",
        "\r\n",
        "  data.to_csv(dataPath + '/ecg_data_v1.csv', header = False, index = False)\r\n",
        "\r\n",
        "\r\n",
        "  val_df, test_df = train_test_split(\r\n",
        "    val_df,\r\n",
        "    test_size=0.33,\r\n",
        "    random_state=RANDOM_SEED\r\n",
        "  )\r\n",
        "\r\n",
        "  def create_dataset(df):\r\n",
        "    sequences = df.astype(np.float32).to_numpy().tolist()\r\n",
        "    dataset = [torch.tensor(s).unsqueeze(1).float() for s in sequences]\r\n",
        "    n_seq, seq_len, n_features = torch.stack(dataset).shape\r\n",
        "    return dataset, seq_len, n_features\r\n",
        "\r\n",
        "\r\n",
        "  train_dataset, seq_len, n_features = create_dataset(train_df)\r\n",
        "  val_dataset, _, _ = create_dataset(val_df)\r\n",
        "  test_normal_dataset, _, _ = create_dataset(test_df)\r\n",
        "  test_anomaly_dataset, _, _ = create_dataset(anomaly_df)\r\n",
        "\r\n",
        "  import torch\r\n",
        "  from torch.utils.data import Dataset, DataLoader\r\n",
        "  import pandas as pd\r\n",
        "\r\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "  class ECGData(Dataset):\r\n",
        "    #This is the class for teh ECG Data that we need to load, transform and then use in teh dataloader.\r\n",
        "    def __init__(self,source_file,transform = None):\r\n",
        "      self.source_file = source_file\r\n",
        "      self.data  = pd.read_csv(source_file, header = None)\r\n",
        "      self.transform = transform\r\n",
        "      \r\n",
        "    def __len__(self):\r\n",
        "      return self.data.shape[0]\r\n",
        "      \r\n",
        "    def __getitem__(self,idx):\r\n",
        "      \r\n",
        "      sample = self.data.iloc[idx]\r\n",
        "      \r\n",
        "      if self.transform:\r\n",
        "          sample = self.transform(sample)\r\n",
        "          \r\n",
        "      return sample   \r\n",
        "\r\n",
        "  \"\"\"Including the function that will transform the dataframe to a pytorch tensor\"\"\"\r\n",
        "  class PD_to_Tensor(object):\r\n",
        "      def __call__(self,sample):\r\n",
        "        return torch.tensor(sample.values).to(device=device, dtype=float)\r\n",
        "\r\n",
        "\r\n",
        "  print(len(train_dataset))\r\n",
        "  print(seq_len)\r\n",
        "  print(n_features)\r\n",
        "  for n in range(10):\r\n",
        "    plt.plot(train_dataset[n])\r\n",
        "  plt.show()\r\n",
        "\r\n",
        "  def GetECGData(source_file):\r\n",
        "    compose = transforms.Compose([PD_to_Tensor()])\r\n",
        "    return ECGData(source_file ,transform = compose)\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUrjR5i_7d0w"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuPy-NTY7d0z",
        "scrolled": false
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#define _l2normalization\n",
        "def _l2normalize(v, eps=1e-12):\n",
        "    return v / (torch.norm(v) + eps)\n",
        "\n",
        "def max_singular_value(W, u=None, Ip=1):\n",
        "    \"\"\"\n",
        "    power iteration for weight parameter\n",
        "    \"\"\"\n",
        "    #xp = W.data\n",
        "    if not Ip >= 1:\n",
        "        raise ValueError(\"Power iteration should be a positive integer\")\n",
        "    if u is None:\n",
        "        u = torch.FloatTensor(1, W.size(0)).normal_(0, 1).cuda()\n",
        "    _u = u\n",
        "    for _ in range(Ip):\n",
        "        _v = _l2normalize(torch.matmul(_u, W.data), eps=1e-12)\n",
        "        _u = _l2normalize(torch.matmul(_v, torch.transpose(W.data, 0, 1)), eps=1e-12)\n",
        "    sigma = torch.sum(F.linear(_u, torch.transpose(W.data, 0, 1)) * _v)\n",
        "    return sigma, _u\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules import Linear\n",
        "\n",
        "class SNLinear(Linear):\n",
        "    r\"\"\"Applies a linear transformation to the incoming data: :math:`y = Ax + b`\n",
        "       Args:\n",
        "           in_features: size of each input sample\n",
        "           out_features: size of each output sample\n",
        "           bias: If set to False, the layer will not learn an additive bias.\n",
        "               Default: ``True``\n",
        "       Shape:\n",
        "           - Input: :math:`(N, *, in\\_features)` where :math:`*` means any number of\n",
        "             additional dimensions\n",
        "           - Output: :math:`(N, *, out\\_features)` where all but the last dimension\n",
        "             are the same shape as the input.\n",
        "       Attributes:\n",
        "           weight: the learnable weights of the module of shape\n",
        "               `(out_features x in_features)`\n",
        "           bias:   the learnable bias of the module of shape `(out_features)`\n",
        "           W(Tensor): Spectrally normalized weight\n",
        "           u (Tensor): the right largest singular value of W.\n",
        "       \"\"\"\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(SNLinear, self).__init__(in_features, out_features, bias)\n",
        "        self.register_buffer('u', torch.Tensor(1, out_features).normal_())\n",
        "\n",
        "    @property\n",
        "    def W_(self):\n",
        "        w_mat = self.weight.view(self.weight.size(0), -1)\n",
        "        sigma, _u = max_singular_value(w_mat, self.u)\n",
        "        self.u.copy_(_u)\n",
        "        return self.weight / sigma\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.linear(input, self.W_, self.bias)\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules import Conv1d\n",
        "from torch.nn.modules.utils import _pair\n",
        "\n",
        "class SNConv1d(Conv1d):\n",
        "\n",
        "    r\"\"\"Applies a 2D convolution over an input signal composed of several input\n",
        "    planes.\n",
        "    In the simplest case, the output value of the layer with input size\n",
        "    :math:`(N, C_{in}, H, W)` and output :math:`(N, C_{out}, H_{out}, W_{out})`\n",
        "    can be precisely described as:\n",
        "    .. math::\n",
        "        \\begin{array}{ll}\n",
        "        out(N_i, C_{out_j})  = bias(C_{out_j})\n",
        "                       + \\sum_{{k}=0}^{C_{in}-1} weight(C_{out_j}, k)  \\star input(N_i, k)\n",
        "        \\end{array}\n",
        "    where :math:`\\star` is the valid 2D `cross-correlation`_ operator,\n",
        "    :math:`N` is a batch size, :math:`C` denotes a number of channels,\n",
        "    :math:`H` is a height of input planes in pixels, and :math:`W` is\n",
        "    width in pixels.\n",
        "    | :attr:`stride` controls the stride for the cross-correlation, a single\n",
        "      number or a tuple.\n",
        "    | :attr:`padding` controls the amount of implicit zero-paddings on both\n",
        "    |  sides for :attr:`padding` number of points for each dimension.\n",
        "    | :attr:`dilation` controls the spacing between the kernel points; also\n",
        "      known as the à trous algorithm. It is harder to describe, but this `link`_\n",
        "      has a nice visualization of what :attr:`dilation` does.\n",
        "    | :attr:`groups` controls the connections between inputs and outputs.\n",
        "      `in_channels` and `out_channels` must both be divisible by `groups`.\n",
        "    |       At groups=1, all inputs are convolved to all outputs.\n",
        "    |       At groups=2, the operation becomes equivalent to having two conv\n",
        "                 layers side by side, each seeing half the input channels,\n",
        "                 and producing half the output channels, and both subsequently\n",
        "                 concatenated.\n",
        "            At groups=`in_channels`, each input channel is convolved with its\n",
        "                 own set of filters (of size `out_channels // in_channels`).\n",
        "    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:\n",
        "        - a single ``int`` -- in which case the same value is used for the height and width dimension\n",
        "        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,\n",
        "          and the second `int` for the width dimension\n",
        "    .. note::\n",
        "         Depending of the size of your kernel, several (of the last)\n",
        "         columns of the input might be lost, because it is a valid `cross-correlation`_,\n",
        "         and not a full `cross-correlation`_.\n",
        "         It is up to the user to add proper padding.\n",
        "    .. note::\n",
        "         The configuration when `groups == in_channels` and `out_channels = K * in_channels`\n",
        "         where `K` is a positive integer is termed in literature as depthwise convolution.\n",
        "         In other words, for an input of size :math:`(N, C_{in}, H_{in}, W_{in})`, if you want a\n",
        "         depthwise convolution with a depthwise multiplier `K`,\n",
        "         then you use the constructor arguments\n",
        "         :math:`(in\\_channels=C_{in}, out\\_channels=C_{in} * K, ..., groups=C_{in})`\n",
        "    Args:\n",
        "        in_channels (int): Number of channels in the input image\n",
        "        out_channels (int): Number of channels produced by the convolution\n",
        "        kernel_size (int or tuple): Size of the convolving kernel\n",
        "        stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
        "        padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0\n",
        "        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
        "        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n",
        "        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n",
        "    Shape:\n",
        "        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`\n",
        "        - Output: :math:`(N, C_{out}, H_{out}, W_{out})` where\n",
        "          :math:`H_{out} = floor((H_{in}  + 2 * padding[0] - dilation[0] * (kernel\\_size[0] - 1) - 1) / stride[0] + 1)`\n",
        "          :math:`W_{out} = floor((W_{in}  + 2 * padding[1] - dilation[1] * (kernel\\_size[1] - 1) - 1) / stride[1] + 1)`\n",
        "    Attributes:\n",
        "        weight (Tensor): the learnable weights of the module of shape\n",
        "                         (out_channels, in_channels, kernel_size[0], kernel_size[1])\n",
        "        bias (Tensor):   the learnable bias of the module of shape (out_channels)\n",
        "        W(Tensor): Spectrally normalized weight\n",
        "        u (Tensor): the right largest singular value of W.\n",
        "    .. _cross-correlation:\n",
        "        https://en.wikipedia.org/wiki/Cross-correlation\n",
        "    .. _link:\n",
        "        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
        "    \"\"\"\n",
        "\n",
        "#padding_mode: str='zeros'\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
        "        kernel_size = (kernel_size)\n",
        "        stride = (stride)\n",
        "        padding = (padding)\n",
        "        dilation = (dilation)\n",
        "        super(SNConv1d, self).__init__(\n",
        "            in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
        "        self.register_buffer('u', torch.Tensor(1, out_channels).normal_())\n",
        "\n",
        "    @property\n",
        "    def W_(self):\n",
        "        w_mat = self.weight.view(self.weight.size(0), -1)\n",
        "        sigma, _u = max_singular_value(w_mat, self.u)\n",
        "        self.u.copy_(_u)\n",
        "        return self.weight / sigma\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.conv1d(input, self.W_, self.bias, self.stride,\n",
        "                        self.padding, self.dilation, self.groups)\n",
        "        \n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpIwLbfn7d04"
      },
      "source": [
        "### Models (Generator and Discriminator)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy8DFFaO7d05",
        "scrolled": false
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def l2normalize(v, eps=1e-12):\n",
        "    return v / (v.norm() + eps)\n",
        "\n",
        "\n",
        "class SpectralNorm(nn.Module):\n",
        "    def __init__(self, module, name='weight', power_iterations=1):\n",
        "        super(SpectralNorm, self).__init__()\n",
        "        self.module = module\n",
        "        self.name = name\n",
        "        self.power_iterations = power_iterations\n",
        "        if not self._made_params():\n",
        "            self._make_params()\n",
        "\n",
        "    def _update_u_v(self):\n",
        "        u = getattr(self.module, self.name + \"_u\")\n",
        "        v = getattr(self.module, self.name + \"_v\")\n",
        "        w = getattr(self.module, self.name + \"_bar\")\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        for _ in range(self.power_iterations):\n",
        "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
        "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
        "\n",
        "        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n",
        "        sigma = u.dot(w.view(height, -1).mv(v))\n",
        "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
        "\n",
        "    def _made_params(self):\n",
        "        try:\n",
        "            u = getattr(self.module, self.name + \"_u\")\n",
        "            v = getattr(self.module, self.name + \"_v\")\n",
        "            w = getattr(self.module, self.name + \"_bar\")\n",
        "            return True\n",
        "        except AttributeError:\n",
        "            return False\n",
        "\n",
        "\n",
        "    def _make_params(self):\n",
        "        w = getattr(self.module, self.name)\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        width = w.view(height, -1).data.shape[1]\n",
        "\n",
        "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
        "        u.data = l2normalize(u.data)\n",
        "        v.data = l2normalize(v.data)\n",
        "        w_bar = Parameter(w.data)\n",
        "\n",
        "        del self.module._parameters[self.name]\n",
        "\n",
        "        self.module.register_parameter(self.name + \"_u\", u)\n",
        "        self.module.register_parameter(self.name + \"_v\", v)\n",
        "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
        "\n",
        "\n",
        "    def forward(self, *args):\n",
        "        self._update_u_v()\n",
        "        return self.module.forward(*args)\n",
        "\n",
        "class MinibatchDiscrimination(torch.nn.Module):\n",
        "   def __init__(self,input_features,output_features,minibatch_normal_init, hidden_features=16):\n",
        "      super(MinibatchDiscrimination,self).__init__()\n",
        "      \n",
        "      self.input_features = input_features\n",
        "      self.output_features = output_features\n",
        "      self.hidden_features = hidden_features\n",
        "      self.T = torch.nn.Parameter(torch.randn(self.input_features,self.output_features, self.hidden_features))\n",
        "      if minibatch_normal_init == True:\n",
        "        nn.init.normal_(self.T, 0,1)\n",
        "      \n",
        "   def forward(self,x):\n",
        "      M = torch.mm(x,self.T.view(self.input_features,-1))\n",
        "      M = M.view(-1, self.output_features, self.hidden_features).unsqueeze(0)\n",
        "      M_t = M.permute(1, 0, 2, 3)\n",
        "      # Broadcasting reduces the matrix subtraction to the form desired in the paper\n",
        "      out = torch.sum(torch.exp(-(torch.abs(M - M_t).sum(3))), dim=0) - 1\n",
        "      \n",
        "      return torch.cat([x, out], 1)\n",
        "    \n",
        "\n",
        "# Use minibatch = 0 for no minibatch discriminiation layer to be used in the architecture. If minibatch > 0, then minibatch is the number of output dimensions of the MBD layer.\n",
        "class Discriminator(torch.nn.Module):\n",
        "  def __init__(self,seq_length,batch_size,minibatch_normal_init, n_features = 1, num_cv = 1, minibatch = 0, cv1_out= 10, cv1_k = 3, cv1_s = 4, p1_k = 3, p1_s = 3, cv2_out = 10, cv2_k = 3, cv2_s = 3 ,p2_k = 3, p2_s = 3):\n",
        "      super(Discriminator,self).__init__()\n",
        "      self.n_features = n_features\n",
        "      self.seq_length = seq_length\n",
        "      self.batch_size = batch_size\n",
        "      self.num_cv = num_cv\n",
        "      self.minibatch = minibatch\n",
        "      self.cv1_dims = int((((((seq_length - cv1_k)/cv1_s) + 1)-p1_k)/p1_s)+1)\n",
        "      self.cv2_dims = int((((((self.cv1_dims - cv2_k)/cv2_s) + 1)-p2_k)/p2_s)+1)\n",
        "      self.cv1_out = cv1_out\n",
        "      self.cv2_out = cv2_out\n",
        "      \n",
        "\n",
        "      self.dropout = torch.nn.Dropout(0.05)\n",
        "      conv1_features = self.n_features\n",
        "\n",
        "\n",
        "      self.conv1 = SNConv1d(conv1_features, cv1_out, cv1_k, cv1_s, 1, 1, 1 , True)\n",
        "      self.conv2 = SNConv1d(cv1_out, cv2_out, cv2_k, cv1_s, 1, 1, 1 ,True)\n",
        "\n",
        "      self.out1 = SNLinear(cv2_out*12, cv2_out*12)\n",
        "      self.out2 = SNLinear(cv2_out*12, 1)\n",
        "\n",
        "      self.lr = nn.LeakyReLU(0.1, inplace=True)\n",
        "\n",
        "\n",
        "      self.cv1 = SpectralNorm(nn.Conv1d(conv1_features, cv1_out, cv1_k, cv1_s,1))\n",
        "      self.cv2 = SpectralNorm(nn.Conv1d(cv1_out, cv2_out, cv2_k, cv2_s,1))\n",
        "      self.o1 = SpectralNorm(nn.Linear(cv2_out*12, cv2_out*12))\n",
        "      self.o2 = SpectralNorm(nn.Linear(cv2_out*12, 1))\n",
        "      self.l = nn.LeakyReLU(0.1)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "      self.batch_size = x.size(0)\n",
        "      x = x.view(self.batch_size,1,self.seq_length) \n",
        "\n",
        "      x = self.l(self.cv1(x))\n",
        "      x = self.l(self.cv2(x))\n",
        "      x = x.view(self.batch_size, -1)\n",
        "      x = self.l(self.o1(x))\n",
        "      x = self.dropout(x)\n",
        "      x = self.o2(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "      #print(f'in {x.shape}')\n",
        "      x = self.lr(self.conv1(x))\n",
        "      #print(f'aa {x.shape}')\n",
        "      x = self.lr(self.conv2(x))\n",
        "      #print(f'ab {x.shape}')\n",
        "      #print(f'abp {x.shape}')\n",
        "      x = x.view(self.batch_size, -1)\n",
        "   #  print(f'conv2 out {x.shape}') #64, 176\n",
        "\n",
        "      x = self.lr(self.out1(x))\n",
        "      x = self.dropout(x)\n",
        "      #print(f'la {x.shape}')\n",
        "      x = self.out2(x)\n",
        "      #print(f'lb {x.shape}')\n",
        "      return x\n",
        "      \n",
        "\n",
        "class DLSTM(torch.nn.Module):\n",
        "  #seq_length not important\n",
        "  def __init__(self):\n",
        "      super(DLSTM,self).__init__()\n",
        "      self.n_features = 1\n",
        "      self.hidden_dim = 50\n",
        "      self.num_layers = 2\n",
        "      self.seq_length = 50\n",
        "      self.batch_size = 64\n",
        "      self.bidirectional = False\n",
        "\n",
        "      #self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "      \n",
        "      #Checking if the architecture uses a BiLSTM and setting the output parameters as appropriate.\n",
        "      if self.bidirectional == True:\n",
        "        self.num_dirs = 2\n",
        "      else:\n",
        "        self.num_dirs = 1\n",
        "      \n",
        "      \n",
        "      self.layer1 = torch.nn.LSTM(input_size = self.n_features, hidden_size = self.hidden_dim, num_layers = self.num_layers,batch_first = True, bidirectional = self.bidirectional )\n",
        "      self.out = torch.nn.Linear(self.hidden_dim,50) # to make sure the output is between 0 and 1 - removed ,torch.nn.Sigmoid()\n",
        "  \n",
        "  def forward(self,x):\n",
        "      self.seq_length = x.size(1)\n",
        "      self.batch_size = x.size(0)\n",
        "\n",
        "      h_0 = torch.zeros(self.num_layers*self.num_dirs, self.batch_size, self.hidden_dim, requires_grad=True).to(device)\n",
        "      c_0 = torch.zeros(self.num_layers*self.num_dirs, self.batch_size, self.hidden_dim, requires_grad=True).to(device)\n",
        "      #x, _ = self.layer1(x.view(self.batch_size,self.seq_length,1), (h_0, c_0)) \n",
        "      x, _ = self.layer1(x.view(self.batch_size,self.seq_length, self.n_features), (h_0, c_0)) #--------------------------------noise_features ---------------------------------------\n",
        "\n",
        "\n",
        "      if self.bidirectional == True:\n",
        "        x = x.view(x.size(0), x.size(1), 2, -1).sum(2).view(x.size(0), x.size(1), -1)\n",
        "      \n",
        "      #Note that the output of the bidirectional LSTM is in the form (batch_size,seq_lenth,num_dirs*hidden_dim) To separate the directions, we can use \n",
        "      #x.view(self.batch_size,self.seq_length,self.num_dirs, self.hidden_dim)\n",
        "      #x = self.dropout(x)\n",
        "\n",
        "      x = self.out(x)\n",
        "\n",
        "      return x.squeeze() #,hidden\n",
        "\n",
        "class Generator(torch.nn.Module):\n",
        "  #seq_length not important\n",
        "  def __init__(self,seq_length,batch_size,n_features = 1, hidden_dim = 50, num_layers = 2, tanh_output = False, bidirectional = False):\n",
        "      super(Generator,self).__init__()\n",
        "      self.n_features = n_features\n",
        "      self.hidden_dim = hidden_dim\n",
        "      self.num_layers = num_layers\n",
        "      self.seq_length = seq_length\n",
        "      self.batch_size = batch_size\n",
        "      self.tanh_output = tanh_output\n",
        "      self.bidirectional = bidirectional\n",
        "\n",
        "      #self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "      \n",
        "      #Checking if the architecture uses a BiLSTM and setting the output parameters as appropriate.\n",
        "      if self.bidirectional == True:\n",
        "        self.num_dirs = 2\n",
        "      else:\n",
        "        self.num_dirs = 1\n",
        "      \n",
        "      \n",
        "      self.layer1 = torch.nn.LSTM(input_size = self.n_features, hidden_size = self.hidden_dim, num_layers = self.num_layers,batch_first = True, bidirectional = self.bidirectional )\n",
        "      self.out = torch.nn.Linear(self.hidden_dim,1) # to make sure the output is between 0 and 1 - removed ,torch.nn.Sigmoid()\n",
        "  \n",
        "  def forward(self,x):\n",
        "      self.seq_length = x.size(1)\n",
        "      self.batch_size = x.size(0)\n",
        "\n",
        "      h_0 = torch.zeros(self.num_layers*self.num_dirs, self.batch_size, self.hidden_dim, requires_grad=True).to(device)\n",
        "      c_0 = torch.zeros(self.num_layers*self.num_dirs, self.batch_size, self.hidden_dim, requires_grad=True).to(device)\n",
        "      #x, _ = self.layer1(x.view(self.batch_size,self.seq_length,1), (h_0, c_0)) \n",
        "      x, _ = self.layer1(x.view(self.batch_size,self.seq_length, self.n_features), (h_0, c_0)) #--------------------------------noise_features ---------------------------------------\n",
        "\n",
        "\n",
        "      if self.bidirectional == True:\n",
        "        x = x.view(x.size(0), x.size(1), 2, -1).sum(2).view(x.size(0), x.size(1), -1)\n",
        "      \n",
        "      #Note that the output of the bidirectional LSTM is in the form (batch_size,seq_lenth,num_dirs*hidden_dim) To separate the directions, we can use \n",
        "      #x.view(self.batch_size,self.seq_length,self.num_dirs, self.hidden_dim)\n",
        "      #x = self.dropout(x)\n",
        "\n",
        "      x = self.out(x)\n",
        "\n",
        "      return x.squeeze() #,hidden"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krloNMoT7d09"
      },
      "source": [
        "### Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onF_Ajx-7d0-",
        "scrolled": false
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class SineData(Dataset):\n",
        "  #This is the class for teh ECG Data that we need to load, transform and then use in teh dataloader.\n",
        "  def __init__(self,source_file,transform = None):\n",
        "    self.source_file = source_file\n",
        "    self.data  = pd.read_csv(source_file, header = None)\n",
        "    self.transform = transform\n",
        "    \n",
        "  def __len__(self):\n",
        "    return self.data.shape[0]\n",
        "    \n",
        "  def __getitem__(self,idx):\n",
        "    \n",
        "    sample = self.data.iloc[idx]\n",
        "    \n",
        "    if self.transform:\n",
        "        sample = self.transform(sample)\n",
        "        \n",
        "    return sample   \n",
        "\n",
        "\"\"\"Including the function that will transform the dataframe to a pytorch tensor\"\"\"\n",
        "class PD_to_Tensor(object):\n",
        "    def __call__(self,sample):\n",
        "      return torch.tensor(sample.values).to(device=device, dtype=float)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLzzVg_k7d1B"
      },
      "source": [
        "### Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKWu839c7d1C",
        "scrolled": false
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "dataPath = './dataset'  \n",
        "if not os.path.exists(dataPath):\n",
        "        os.makedirs(dataPath)\n",
        "\n",
        "trainingset_size = 64 * 150\n",
        "testset_size = 64 * 20\n",
        "\n",
        "\"\"\"Create a training set of sine waves with 10000 records\"\"\"\n",
        "\n",
        "#a = np.arange(0.1,0.9,0.02)\n",
        "x = np.arange(0,20,0.4)\n",
        "#r = np.arange(2,6.1,0.1)\n",
        "count = 0\n",
        "fs = len(x)\n",
        "y = np.zeros((1,len(x)))\n",
        "\n",
        "for n in range(trainingset_size):\n",
        "  amp = random.uniform(0.1, 0.9)\n",
        "  rad = random.uniform(5, 10)\n",
        "  phase = random.uniform(-fs/0.5*np.pi,fs/0.5*np.pi)\n",
        "  offsetY = 0 #random.uniform(-1, 1)\n",
        "  y = np.append(y,  offsetY + amp*np.sin(((2*np.pi*rad*x)+phase)/fs).reshape((1,len(x))),axis = 0)\n",
        "  \n",
        "# for n in range(20):\n",
        "#    plt.plot(y[n + 1])\n",
        "# plt.show()\n",
        "data = pd.DataFrame(y[1:][:])  \n",
        "data.to_csv(dataPath + '/sinedata_v2.csv', header = False, index = False)\n",
        "\n",
        "\"\"\"Creating a test set of sine waves with 3000 records\"\"\"\n",
        "a = np.arange(0.1,0.9,0.02)\n",
        "x = np.arange(0,20,0.5)\n",
        "r = np.arange(2,6.1,0.1)\n",
        "count = 0\n",
        "fs = len(x)\n",
        "y = np.zeros((1,len(x)))\n",
        "\n",
        "for n in range(testset_size):\n",
        "  amp = a[random.randint(0,len(a)-1)]\n",
        "  rad = r[random.randint(0,len(r)-1)]\n",
        "  phase = random.uniform(-1,1)*np.pi\n",
        "\n",
        "  y = np.append(y,amp*np.sin(((2*np.pi*rad*x)+phase)/fs).reshape((1,len(x))),axis = 0)\n",
        "  \n",
        "data = pd.DataFrame(y[1:][:])  \n",
        "data.to_csv(dataPath + '/sinedata_test_v2.csv', header = False, index = False)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ak6ADH40auP"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Krranegc7d1F"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnxOCeOt7d1F",
        "scrolled": false
      },
      "source": [
        "def noise(batchsize, seq_length, noise_features):\n",
        "    rnoise = torch.ones( (batchsize, seq_length, noise_features)).to(device)\n",
        "    if noise_features == 1:\n",
        "      return torch.rand(batchsize, seq_length, 1)\n",
        "\n",
        "    for b in range(batchsize):\n",
        "        randoms = np.ones(noise_features)\n",
        "        for nf in range(noise_features):\n",
        "            randoms[nf] = random.uniform(-1,1)\n",
        "        for i in range(seq_length):\n",
        "            for nf in range(noise_features):\n",
        "                rnoise[b][i][nf] = randoms[nf]\n",
        "    return rnoise\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def pdist(sample_1, sample_2, norm=2, eps=1e-5):\n",
        "    n_1, n_2 = sample_1.size(0), sample_2.size(0)\n",
        "    norm = float(norm)\n",
        "    \n",
        "    if norm == 2.:\n",
        "        norms_1 = torch.sum(sample_1**2, dim=1, keepdim=True)\n",
        "        norms_2 = torch.sum(sample_2**2, dim=1, keepdim=True)\n",
        "        norms = (norms_1.expand(n_1, n_2) +\n",
        "                 norms_2.transpose(0, 1).expand(n_1, n_2))\n",
        "        distances_squared = norms - 2 * sample_1.mm(sample_2.t())\n",
        "        return torch.sqrt(eps + torch.abs(distances_squared))\n",
        "    else:\n",
        "        dim = sample_1.size(1)\n",
        "        expanded_1 = sample_1.unsqueeze(1).expand(n_1, n_2, dim)\n",
        "        expanded_2 = sample_2.unsqueeze(0).expand(n_1, n_2, dim)\n",
        "        differences = torch.abs(expanded_1 - expanded_2) ** norm\n",
        "        inner = torch.sum(differences, dim=2, keepdim=False)\n",
        "        return (eps + inner) ** (1. / norm)\n",
        "\n",
        "def permutation_test_mat(matrix,n_1,  n_2,  n_permutations, a00=1,  a11=1,  a01=0):\n",
        "    n = n_1 + n_2\n",
        "    pi = np.zeros(n, dtype=np.int8)\n",
        "    pi[n_1:] = 1\n",
        "\n",
        "    larger = 0.\n",
        "    count = 0\n",
        "    \n",
        "    for sample_n in range(1 + n_permutations):\n",
        "        count = 0.\n",
        "        for i in range(n):\n",
        "            for j in range(i, n):\n",
        "                mij = matrix[i, j] + matrix[j, i]\n",
        "                if pi[i] == pi[j] == 0:\n",
        "                    count += a00 * mij\n",
        "                elif pi[i] == pi[j] == 1:\n",
        "                    count += a11 * mij\n",
        "                else:\n",
        "                    count += a01 * mij\n",
        "        if sample_n == 0:\n",
        "            statistic = count\n",
        "        elif statistic <= count:\n",
        "            larger += 1\n",
        "\n",
        "        np.random.shuffle(pi)\n",
        "\n",
        "    return larger / n_permutations\n",
        "\n",
        "\n",
        "class MMDStatistic:\n",
        "    def __init__(self, n_1, n_2):\n",
        "        self.n_1 = n_1\n",
        "        self.n_2 = n_2\n",
        "\n",
        "        # The three constants used in the test.\n",
        "        self.a00 = 1. / (n_1 * (n_1 - 1))\n",
        "        self.a11 = 1. / (n_2 * (n_2 - 1))\n",
        "        self.a01 = - 1. / (n_1 * n_2)\n",
        "\n",
        "    def __call__(self, sample_1, sample_2, alphas, ret_matrix=False):\n",
        "\n",
        "        sample_12 = torch.cat((sample_1, sample_2), 0)\n",
        "        distances = pdist(sample_12, sample_12, norm=2)\n",
        "\n",
        "        kernels = None\n",
        "        for alpha in alphas:\n",
        "            kernels_a = torch.exp(- alpha * distances ** 2)\n",
        "            if kernels is None:\n",
        "                kernels = kernels_a\n",
        "            else:\n",
        "                kernels = kernels + kernels_a\n",
        "\n",
        "        k_1 = kernels[:self.n_1, :self.n_1]\n",
        "        k_2 = kernels[self.n_1:, self.n_1:]\n",
        "        k_12 = kernels[:self.n_1, self.n_1:]\n",
        "\n",
        "        mmd = (2 * self.a01 * k_12.sum() +\n",
        "               self.a00 * (k_1.sum() - torch.trace(k_1)) +\n",
        "               self.a11 * (k_2.sum() - torch.trace(k_2)))\n",
        "        if ret_matrix:\n",
        "            return mmd, kernels\n",
        "        else:\n",
        "            return mmd\n",
        "\n",
        "\n",
        "    def pval(self, distances, n_permutations=1000):\n",
        "        if isinstance(distances, Variable):\n",
        "            distances = distances.data\n",
        "        return permutation_test_mat(distances.cpu().numpy(),\n",
        "                                    self.n_1, self.n_2,\n",
        "                                    n_permutations,\n",
        "                                    a00=self.a00, a11=self.a11, a01=self.a01)\n",
        "\n",
        "\n",
        "\n",
        "def saveModel(path, epoch): \n",
        "    \n",
        "    torch.save({\n",
        "            'generator_state_dict': generator.state_dict(),\n",
        "            'discriminator_state_dict': discriminator.state_dict(),\n",
        "            'optimizerG_state_dict': generator_opt.state_dict(),\n",
        "            'optimizerD_state_dict': discriminator_opt.state_dict(),  \n",
        "            'D_losses': D_losses,\n",
        "            'G_losses': G_losses,\n",
        "            'mmd_list': mmd_list,\n",
        "            'gradient_p': gradient_p,\n",
        "            'series_list': series_list,\n",
        "            'epoch': epoch\n",
        "            }, path +'/epoch'+str(epoch)+'.tar')\n",
        "\n",
        "\n",
        "def pairwisedistances(X,Y,norm=2):\n",
        "    dist = pdist(X,Y,norm)\n",
        "    return np.median(dist.numpy())\n",
        "\n",
        "\n",
        "def GetSineData(source_file):\n",
        "  compose = transforms.Compose([PD_to_Tensor()])\n",
        "  return SineData(source_file ,transform = compose)\n",
        "\n",
        "def compute_grad2(d_out, x_in):\n",
        "    batch_size = x_in.size(0)\n",
        "    grad_dout = autograd.grad(\n",
        "        outputs=d_out.sum(), inputs=x_in,\n",
        "        create_graph=True, retain_graph=True, only_inputs=True\n",
        "    )[0]\n",
        "    grad_dout2 = grad_dout.pow(2)\n",
        "    assert(grad_dout2.size() == x_in.size())\n",
        "    reg = grad_dout2.view(batch_size, -1).sum(1)\n",
        "    return reg\n",
        "\n",
        "def wgan_gp_reg(discriminator, x_real, x_fake, center=0.):\n",
        "    batch_size = x_fake.size(0)\n",
        "    #x_real = x_real.unsqueeze(1)\n",
        "    #x_fake = x_fake.unsqueeze(1)\n",
        "    #print(f'xrea {x_real.shape}')\n",
        "    #print(f'xfake {x_fake.shape}')\n",
        "    eps = torch.rand(batch_size, device=x_fake.device).view(batch_size, 1)\n",
        "    x_interp = (1 - eps) * x_real + eps * x_fake\n",
        "    x_interp = x_interp.detach()\n",
        "    x_interp.requires_grad_()\n",
        "    #print(f'intr {x_interp.shape}')\n",
        "    d_out = discriminator(x_interp)\n",
        "\n",
        "    reg = ( torch.sqrt(compute_grad2(d_out, x_interp) + 1e-12) - center).pow(2).mean()\n",
        "\n",
        "    #print(f'reg: {reg}')\n",
        "    return reg\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDnOaj5T7d1J"
      },
      "source": [
        "### Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ble1AVAh7d1K",
        "scrolled": false
      },
      "source": [
        "\n",
        "\n",
        "        # if lipschitz_constraint == 0:\n",
        "        #     # Calculate interpolation\n",
        "        #     epsilon = torch.rand(sample_size, 1).to(device)\n",
        "        #     epsilon = epsilon.expand_as(real_data)\n",
        "        #     interpolated = epsilon * real_data + (1 - epsilon) * fake_data # interpolates := xhat        \n",
        "        #     interpolated = Variable(interpolated, requires_grad=True).to(device)\n",
        "        #     # Calculate probability of interpolated examples\n",
        "        #     prob_interpolated = discriminator(interpolated)\n",
        "        #     gradients = torch.autograd.grad(outputs=prob_interpolated, inputs=interpolated, grad_outputs=torch.ones_like(prob_interpolated), create_graph=True, retain_graph=True)[0]\n",
        "        #     # set up tensors (torch administrativa)\n",
        "        #     # Gradients have shape (batch_size, num_channels, img_width, img_height),\n",
        "        #     # so flatten to easily take norm per example in batch\n",
        "        #     gradients = gradients.view(sample_size, -1)\n",
        "        #     # Derivatives of the gradient close to 0 can cause problems because of\n",
        "        #     # the square root, so manually calculate norm and add epsilon\n",
        "        #     gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
        "        #     gradient_penalty =penalty_weight * ((gradients_norm - 1) ** 2).mean()        \n",
        "        #     loss_discriminator = loss_discriminator +   gradient_penalty\n",
        "        \n",
        "        # elif lipschitz_constraint == 1:\n",
        "        #         dist = ((real_data-fake_data)**2).sum(1)**0.5\n",
        "        #         lip_est = (critic_out_real.mean()-critic_out_fake.mean()).abs()/(dist+1e-8)\n",
        "        #         lip_loss = penalty_weight*(clip_fn(1.0-lip_est)**2).mean(0).view(1)\n",
        "        #         gradient_penalty = lip_loss\n",
        "        #         loss_discriminator = loss_discriminator + lip_loss\n",
        "        # elif lipschitz_constraint == 3:           \n",
        "        #         interp_alpha.resize_(sample_size, 1)\n",
        "        #         interp_alpha.uniform_()\n",
        "        #         interp_points = Variable((interp_alpha.expand_as(real_data.data)*real_data.data+(1-interp_alpha.expand_as(real_data.data))*fake_data.data), requires_grad=True)\n",
        "        #         errD_interp_vec = discriminator(interp_points)\n",
        "        #         errD_gradient, = torch.autograd.grad(errD_interp_vec.sum(), interp_points, create_graph=True)\n",
        "        #         lip_est = (errD_gradient**2).view(sample_size,-1).sum(1)**0.5 # updated: bug fix: added **0.5\n",
        "        #         lip_loss = penalty_weight*(clip_fn(1.0-lip_est)**2).mean(0).view(1)\n",
        "        #         gradient_penalty = lip_loss\n",
        "        #         loss_discriminator = loss_discriminator + lip_loss\n",
        "        # elif lipschitz_constraint == 4 or lipschitz_constraint == 5:\n",
        "        #     if lipschitz_constraint == 4:\n",
        "        #       # this tries to match DRAGAN\n",
        "        #       perturbation.resize_as_(real_data.data)\n",
        "        #       perturbation.uniform_()\n",
        "        #       perturbation *= 0.5*real_data.data.std()\n",
        "        #       interp_alpha.resize_(sample_size, 1)  \n",
        "        #       interp_alpha.uniform_()\n",
        "        #       perturbation *= interp_alpha.expand_as(perturbation)\n",
        "        #     else:\n",
        "        #       # the choice of perturbation and leaving out alpha differs from the DRAGAN article\n",
        "        #       perturbation.resize_as_(real_data.data)\n",
        "        #       perturbation.normal_()\n",
        "        #       perturbation *= 0.25*real_data.data.std(0).expand_as(perturbation)\n",
        "\n",
        "\n",
        "        #     perturbation += real_data.data\n",
        "        #     interp_points = Variable(perturbation, requires_grad=True)\n",
        "        #     errD_interp_vec = discriminator(interp_points)\n",
        "        #     errD_gradient, = torch.autograd.grad(errD_interp_vec.sum(), interp_points, create_graph=True)\n",
        "        #     lip_est = (errD_gradient**2).view(sample_size,-1).sum(1)**0.5\n",
        "        #     lip_loss = penalty_weight*(clip_fn(1.0-lip_est)**2).mean(0).view(1)\n",
        "        #     gradient_penalty = lip_loss\n",
        "        #     loss_discriminator = loss_discriminator + lip_loss\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h6Ms-od7d1O"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "31473c81c5fb4ff9b304d7a7cf8589a7",
            "c61d53f6c26d439bbc3defc2808ed879",
            "ec026d8279634565b4c2e2a14a95040e",
            "3e83d2b996c548c9b0785038803ea6c7",
            "d0ca1ef722f84350b95f176ff80d98c4",
            "97c959d4148743d38c0fb8e5c5095481",
            "b03acd29928d4c90acddd9b51184f641",
            "c5cc79d68e3947519db8e5ba5638694b"
          ]
        },
        "id": "FVFu3fV77d1P",
        "scrolled": false,
        "outputId": "9f9968e2-ced3-462f-ae86-13afccda4e40"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f'Using : {device}')\n",
        "    print(torch.cuda.get_device_name(device))\n",
        "else :\n",
        "    print(f'Using : {device}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Creating the training set of sine signals\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "source_filename =  dataPath + '/sinedata_v2.csv'\n",
        "sine_data = GetSineData(source_file = source_filename)\n",
        "data_loader = torch.utils.data.DataLoader(sine_data, batch_size=sample_size, shuffle=True, drop_last=True)\n",
        "seq_length = sine_data[0].size()[0] #Number of features\n",
        "\n",
        "if ecg:\n",
        "  ecg_path =  dataPath + '/ecg_data_v1.csv'\n",
        "  sine_data = GetECGData(source_file = ecg_path)\n",
        "  data_loader = torch.utils.data.DataLoader(sine_data, batch_size=sample_size, shuffle=True)\n",
        "  seq_length = 140\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Num batches\n",
        "num_batches = len(data_loader)\n",
        "\n",
        "\"\"\"Creating the Test Set\"\"\"\n",
        "test_filename = dataPath + '/sinedata_test_v2.csv' #'./sinedata_test_v2.csv'\n",
        "\n",
        "sine_data_test = GetSineData(source_file = test_filename)\n",
        "data_loader_test = torch.utils.data.DataLoader(sine_data_test, batch_size=sample_size, shuffle=True)\n",
        "\n",
        "\n",
        "\"\"\" Evaluation of GAN with 1 CNN Layer in Discriminator\n",
        "##Generator and Discriminator training phase\n",
        "\"\"\"\n",
        "#minibatch_out = [5] #[0,3,5,8,10]\n",
        "#for minibatch_layer in tq.tqdm(minibatch_out):\n",
        "#path = \"./output/Run_\"+str(today.strftime(\"%d_%m_%Y\"))+\"_\"+ str(datetime.datetime.now().time()).split('.')[0]\n",
        "\n",
        "\n",
        "if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Initialising the generator and discriminator\n",
        "generator = Generator(seq_length,sample_size,n_features = noise_features, hidden_dim =  hidden_nodes_g,num_layers= layers, tanh_output = tanh_layer, bidirectional = bidir).to(device)\n",
        "discriminator = Discriminator(seq_length, sample_size ,minibatch_normal_init = minibatch_normal_init_, minibatch = minibatch_layer,num_cv = num_cvs, cv1_out = cv1_out,cv1_k = cv1_k, cv1_s = cv1_s, p1_k = p1_k, p1_s = p1_s, cv2_out= cv2_out, cv2_k = cv2_k, cv2_s = cv2_s, p2_k = p2_k, p2_s = p2_s).to(device)\n",
        "#discriminator = DLSTM()\n",
        "\n",
        "#Defining optimizer\n",
        "if opt_adam:\n",
        "    discriminator_opt = torch.optim.Adam(  filter(lambda p: p.requires_grad, discriminator.parameters())   ,lr = lr_d, betas=[beta1, beta2])\n",
        "    generator_opt = torch.optim.Adam(generator.parameters(),lr = lr_g,  betas=[beta1, beta2])\n",
        "else:\n",
        "    discriminator_opt = torch.optim.RMSprop(discriminator.parameters(), lr=learning_rate)\n",
        "    generator_opt = torch.optim.RMSprop(generator.parameters(), lr=learning_rate)\n",
        "\n",
        "gradient_p = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "mmd_list = []\n",
        "series_list = np.zeros((1,seq_length))\n",
        "\n",
        "\n",
        "if loadModel:\n",
        "    checkpoint = torch.load(loadPath)#torch.load(loadPath)\n",
        "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
        "    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
        "    generator_opt.load_state_dict(checkpoint['optimizerG_state_dict'])\n",
        "    discriminator_opt.load_state_dict(checkpoint['optimizerD_state_dict'])\n",
        "    gradient_p = checkpoint['gradient_p']\n",
        "    G_losses = checkpoint['G_losses']\n",
        "    D_losses = checkpoint['D_losses']\n",
        "    mmd_list = checkpoint['mmd_list']\n",
        "    series_list = checkpoint['series_list']\n",
        "    #epoch_loaded = checkpoint['epoch']\n",
        "    \n",
        "    plt.plot(G_losses)\n",
        "    plt.plot(D_losses)\n",
        "    plt.plot(mmd_list)\n",
        "    plt.plot(gradient_p)\n",
        "    plt.show()\n",
        "    print('Models loaded!')\n",
        "\n",
        "\n",
        "\n",
        "wandb.watch([generator, discriminator], log='all', log_freq=80*1)\n",
        "#wandb.watch(, log='all',  log_freq=80*2)\n",
        "\n",
        "generator.train()\n",
        "discriminator.train()\n",
        "\n",
        "gradient_penalty = torch.ones(1).to(device)\n",
        "\n",
        "\n",
        "input1 = torch.FloatTensor(sample_size, 2).to(device)\n",
        "input2 = torch.FloatTensor(sample_size, 2).to(device)\n",
        "interp_alpha = torch.FloatTensor(sample_size, 1).to(device)\n",
        "\n",
        "fixed_noise = torch.FloatTensor(sample_size, 2).normal_(0, 1).to(device)\n",
        "perturbation = torch.FloatTensor(sample_size,2).to(device)   \n",
        "\n",
        "\n",
        "itera = -1\n",
        "\n",
        "for epoch in tq.tqdm(range(num_epoch)):\n",
        "    if epoch <= epoch_loaded: continue\n",
        "    for n_batch, sample_data in enumerate(data_loader): # enumerate(tq.tqdm(data_loader)): #tq.tqdm(enumerate(data_loader)): \n",
        "\n",
        "      lr_g = lr_g * lr_decrease_factor\n",
        "      lr_d = lr_d * lr_decrease_factor\n",
        "    \n",
        "      itera = itera + 1\n",
        "\n",
        "      for p in discriminator.parameters():  # reset requires_grad\n",
        "        p.requires_grad = True  # they are set to False below in netG update\n",
        "\n",
        "      for d in range(D_rounds):\n",
        "        \n",
        "        #Train Discriminator on Fake Data\n",
        "        discriminator.zero_grad()\n",
        "\n",
        "        #Generating the noise and label data\n",
        "        #noise_sample = Variable(noise(len(sample_data),seq_length))\n",
        "        noise_sample = noise(len(sample_data), seq_length,noise_features )\n",
        "\n",
        "        #Use this line if generator outputs hidden states: fake_data, (h_g_n,c_g_n) = generator.forward(noise_sample,h_g)\n",
        "        #with torch.no_grad():\n",
        "        fake_data = generator(noise_sample)#.detach()\n",
        "\n",
        "        critic_out_fake = discriminator(fake_data)\n",
        "        # tr = 0\n",
        "        # for n in sample_data:\n",
        "        #   if tr < 10:\n",
        "        #     plt.plot(n)\n",
        "        #     tr = 1 + tr\n",
        "        # plt.show()\n",
        "        #Train Discriminator on Real Data \n",
        "        real_data = sample_data.requires_grad_().to(device=device, dtype=torch.float)\n",
        "        #real_data = sample_data.to(device=device, dtype=torch.float)\n",
        "\n",
        "\n",
        "        critic_out_real  = discriminator(real_data)\n",
        "\n",
        "\n",
        "        loss_discriminator = critic_out_fake.mean() - critic_out_real.mean()\n",
        "        \n",
        "\n",
        "        if lipschitz_constraint == 10:\n",
        "            gradient_penalty = penalty_weight * wgan_gp_reg(discriminator, real_data, fake_data, center)\n",
        "            #loss_discriminator = loss_discriminator #+  gradient_penalty\n",
        "\n",
        "            #print(gradient_penalty)\n",
        "\n",
        "\n",
        "        loss_discriminator.backward()\n",
        "        discriminator_opt.step() #Updating the weights based on the predictions for both real and fake calculations.\n",
        "          \n",
        "          \n",
        "    \n",
        "\n",
        "      #Train Generator  \n",
        "      for p in discriminator.parameters():  # reset requires_grad\n",
        "        p.requires_grad = False  # they are set to False below in netG update\n",
        "      #for g in range(G_rounds):\n",
        "\n",
        "\n",
        "      generator.zero_grad()\n",
        "\n",
        "      #noise_sample = Variable(noise(len(sample_data), seq_length))\n",
        "      noise_sample = noise(len(sample_data), seq_length, noise_features)\n",
        "      #Use this line if generator outputs hidden states: gen_fake_data, (h_g_n,c_g_n) = generator.forward(noise_sample,h_g)\n",
        "      fake_data = generator(noise_sample)\n",
        "      critic_out_fake = discriminator(fake_data)\n",
        "      loss_generator = - critic_out_fake.mean()\n",
        "      #loss_generator = torch.mean(critic_out_fake)\n",
        "      loss_generator.backward() #loss_generator.backward(mone) #https://medium.com/@zhang_yang/the-gradient-argument-in-pytorchs-backward-function-explained-by-examples-68f266950c29\n",
        "      #error_gen = loss_1(y_pred_gen,torch.ones([len(sample_data),1]).to(device))\n",
        "      #error_gen.backward()\n",
        "      generator_opt.step()\n",
        "        \n",
        "      gradient_p.append(gradient_penalty.item())\n",
        "      G_losses.append(loss_generator.item())\n",
        "      D_losses.append((loss_discriminator).item())\n",
        "\n",
        "    wandb.log({\"criticReal\":critic_out_real.cpu(), \"criticFake\":critic_out_fake.cpu()  , \"Critic\": loss_discriminator.cpu(), \"Generator\": loss_generator.cpu(), \"GradientPenalty\": gradient_penalty.cpu()})\n",
        "\n",
        "    with torch.no_grad():\n",
        "        discriminator.eval()\n",
        "        generator.eval()\n",
        "        fake = generator(noise(len(sample_data), seq_length, noise_features)).detach().cpu()\n",
        "        fd = discriminator(fake.to(device)).detach().cpu()                \n",
        "        num_samples = 10\n",
        "        fig = go.Figure()\n",
        "        for i in range(num_samples):\n",
        "            fig.add_trace(go.Scatter(x=np.arange(seq_length), y=fake[i].cpu(),\n",
        "                mode='lines',\n",
        "                name='s'+str(i)))\n",
        "        wandb.log({'Epoch ' + str(epoch): fig})\n",
        "        \n",
        "        #real:\n",
        "        if epoch == 1:\n",
        "          fig = go.Figure()\n",
        "          for i in range(num_samples):\n",
        "              fig.add_trace(go.Scatter(x=np.arange(seq_length), y=real_data[i].cpu(),\n",
        "                  mode='lines',\n",
        "                  name='real'))\n",
        "          wandb.log({'Real ': fig})\n",
        "        #####################################################################################################################################\n",
        "        avg_size = 500\n",
        "\n",
        "        rnoise = torch.FloatTensor(avg_size, seq_length).uniform_(-1, 1)\n",
        "        line = torch.zeros(avg_size, seq_length) \n",
        "        for i, s in enumerate(line): \n",
        "          line[i] = s + random.uniform(-1, 1)\n",
        "        def sineWave1(size = 1, rand=1, amp=0, rad=0, phase=0, offsetY=0):\n",
        "          res = torch.zeros(size, 50)\n",
        "          for t in range(size):\n",
        "            x = np.arange(0,20,0.4)\n",
        "            count = 0\n",
        "            fs = len(x)\n",
        "            if rand:\n",
        "              amp = random.uniform(0.1*1, 0.9*1)\n",
        "              rad = random.uniform(5, 10)\n",
        "              phase = random.uniform(-fs/0.5*np.pi,fs/0.5*np.pi)\n",
        "              offsetY =  0# random.uniform(-1, 1) *1\n",
        "          \n",
        "            y = offsetY + amp*np.sin(((2*np.pi*rad*x)+phase)/fs) \n",
        "            if 0:\n",
        "              for i, s in enumerate(y): \n",
        "                y[i] = s + random.uniform(-0.05, 0.001*i)\n",
        "            y = torch.from_numpy(y).to(dtype=torch.float)\n",
        "            res[t] = y\n",
        "\n",
        "          return res.unsqueeze(2)\n",
        "\n",
        "        sine = sineWave1(avg_size)\n",
        "\n",
        "        fake = generator(noise(avg_size, seq_length, noise_features))\n",
        "        real_out = discriminator(sine)\n",
        "        rnoise_out = discriminator(rnoise)\n",
        "        fake_out = discriminator(fake)\n",
        "        line_out = discriminator(line)\n",
        "\n",
        "        print(f'real input: {real_out.mean():.8f} real std: {real_out.std() }fake input: {fake_out.mean():.8f}  random input: {rnoise_out.mean():.8f} line input: {line_out.mean():.8f}')\n",
        "\n",
        "        data = [real_out.detach().squeeze(1).numpy(), rnoise_out.detach().squeeze(1).numpy(), line_out.detach().squeeze(1).numpy(), fake_out.detach().squeeze(1).numpy()]\n",
        "        group_labels = ['real', 'noise', 'line', 'generator']\n",
        "\n",
        "        # Create distplot with custom bin_size\n",
        "        fig = ff.create_distplot(data, group_labels, bin_size=math.pow(0.5/ math.log(avg_size),2))\n",
        "        #fig.show()\n",
        "        wandb.log({'CriticOut ' + str(epoch): fig})\n",
        "\n",
        "                \n",
        "        #####################################################################################################################################\n",
        "\n",
        "\n",
        "        discriminator.train()\n",
        "        generator.train()\n",
        "\n",
        "\n",
        "        \n",
        "                 \n",
        "\n",
        "        series_list = np.append(series_list,fake[0].numpy().reshape((1,seq_length)),axis=0)\n",
        "\n",
        "        #Saving the parameters of the model to file for each epoch\n",
        "        if epoch % 5 == 0:\n",
        "          saveModel(path, epoch)\n",
        "          saveModel('/content/', 999)\n",
        "          wandb.save('models.tar', base_path='/content')\n",
        "\n",
        "                 \n",
        "\n",
        "        series_list = np.append(series_list,fake[0].numpy().reshape((1,seq_length)),axis=0)\n",
        "\n",
        "        #Saving the parameters of the model to file for each epoch\n",
        "        saveModel(path, epoch)\n",
        "        saveModel('/content/', 999)\n",
        "        wandb.save('epoch999.tar', base_path='/content')\n",
        "\n",
        "print('Training Complete!')\n",
        "modelName = 'modelEpoch199.tar'\n",
        "saveModel('/content/', 199)\n",
        "wandb.save(modelName, base_path='/content')\n",
        "\n",
        "print(f'Model {modelName} Saved!')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using : cpu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31473c81c5fb4ff9b304d7a7cf8589a7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "real input: -0.07378540 real std: 0.8663672804832458fake input: -1.50510430  random input: -0.11205348 line input: 0.53075689\n",
            "real input: -3.74167109 real std: 0.6623743772506714fake input: -5.43912888  random input: -3.77001047 line input: -3.88047409\n",
            "real input: -0.90804911 real std: 0.8259062767028809fake input: -2.33670235  random input: -0.93922669 line input: -1.09440529\n",
            "real input: -1.52961385 real std: 0.9349216222763062fake input: -3.36600304  random input: -1.48538995 line input: -1.79905427\n",
            "real input: -1.20950925 real std: 0.9636860489845276fake input: -3.15275502  random input: -1.11510062 line input: -1.41923046\n",
            "real input: -1.10475504 real std: 0.9295527338981628fake input: -2.96067858  random input: -1.11261451 line input: -1.41728020\n",
            "real input: -0.88136047 real std: 0.9711174964904785fake input: -2.83292770  random input: -0.86421663 line input: -1.19618452\n",
            "real input: -0.38411817 real std: 1.0026293992996216fake input: -2.36785913  random input: -0.40848726 line input: -0.54839635\n",
            "real input: -0.60586476 real std: 1.1289795637130737fake input: -2.73902702  random input: -0.63173831 line input: -1.08353794\n",
            "real input: -0.14630203 real std: 0.923839271068573fake input: -1.88140833  random input: -0.10840598 line input: -0.70542848\n",
            "real input: 0.51345879 real std: 1.0923645496368408fake input: -1.66427410  random input: 0.49446666 line input: 0.03274897\n",
            "real input: 0.72321004 real std: 1.1092352867126465fake input: -1.64647901  random input: 0.54101354 line input: 0.08724837\n",
            "real input: 0.56639493 real std: 1.0899664163589478fake input: -1.50830197  random input: 0.52067226 line input: 0.17605963\n",
            "real input: 0.25055307 real std: 1.0790938138961792fake input: -1.81615436  random input: 0.28042927 line input: 0.05136685\n",
            "real input: 0.97008288 real std: 1.1409783363342285fake input: -1.23441696  random input: 0.94763887 line input: 0.54864609\n",
            "real input: 3.01571202 real std: 0.9915882349014282fake input: 0.70469075  random input: 2.82614613 line input: 2.12814569\n",
            "real input: 1.35282886 real std: 1.0447711944580078fake input: -0.57476813  random input: 1.32504702 line input: 0.39480048\n",
            "real input: 0.02837105 real std: 0.9708442091941833fake input: -1.59814215  random input: -0.11992542 line input: -1.36877024\n",
            "real input: 0.86371547 real std: 1.2464014291763306fake input: -0.89186919  random input: 0.93936902 line input: -1.00232720\n",
            "real input: -2.28634429 real std: 0.6983690857887268fake input: -3.04504561  random input: -2.26465273 line input: -3.48641753\n",
            "real input: -1.88297951 real std: 0.6621405482292175fake input: -2.53690553  random input: -1.94220877 line input: -2.73660684\n",
            "real input: -1.59858644 real std: 0.5477434992790222fake input: -2.07419157  random input: -1.69103849 line input: -2.26237988\n",
            "real input: -1.98201275 real std: 0.4323575794696808fake input: -2.48250008  random input: -2.14935517 line input: -2.64532661\n",
            "real input: -2.96325636 real std: 0.5023248791694641fake input: -3.40350652  random input: -3.10479498 line input: -3.53716564\n",
            "real input: -3.31254864 real std: 0.3960685431957245fake input: -3.73775077  random input: -3.47367263 line input: -3.84282327\n",
            "real input: -2.80569339 real std: 0.30865684151649475fake input: -3.20467806  random input: -2.90980196 line input: -3.31319213\n",
            "real input: -3.60741091 real std: 0.35884666442871094fake input: -3.96451283  random input: -3.69065881 line input: -4.01053953\n",
            "real input: -3.00528550 real std: 0.3584457337856293fake input: -3.37246776  random input: -3.16779351 line input: -3.55020857\n",
            "real input: -2.62686467 real std: 0.4375421106815338fake input: -3.00223422  random input: -2.78990531 line input: -3.09815526\n",
            "real input: -3.29866123 real std: 0.32593783736228943fake input: -3.67903256  random input: -3.45144749 line input: -3.77688169\n",
            "real input: -3.69499063 real std: 0.36937227845191956fake input: -4.04302645  random input: -3.78367448 line input: -4.15203381\n",
            "real input: -3.40319395 real std: 0.3670231103897095fake input: -3.76560521  random input: -3.59508395 line input: -3.90587902\n",
            "real input: -3.00711489 real std: 0.47342270612716675fake input: -3.29383326  random input: -3.11907697 line input: -3.50023150\n",
            "real input: -5.26608324 real std: 0.3227130174636841fake input: -5.57237720  random input: -5.31419659 line input: -5.63139248\n",
            "real input: -6.09582663 real std: 0.35303112864494324fake input: -6.39399481  random input: -6.21557522 line input: -6.54008198\n",
            "real input: -6.24869537 real std: 0.395861953496933fake input: -6.48470783  random input: -6.40763330 line input: -6.70237684\n",
            "real input: -5.25308371 real std: 0.46958422660827637fake input: -5.49388790  random input: -5.23726749 line input: -5.51317835\n",
            "real input: -5.82635689 real std: 0.3364131450653076fake input: -6.12994099  random input: -5.97581387 line input: -6.20984507\n",
            "real input: -3.92775750 real std: 0.3441625237464905fake input: -4.24044943  random input: -4.00869036 line input: -4.20016623\n",
            "real input: -5.52884340 real std: 0.35035473108291626fake input: -5.81243515  random input: -5.63445282 line input: -5.80007601\n",
            "real input: -6.29059935 real std: 0.40975797176361084fake input: -6.43522978  random input: -6.37055254 line input: -6.52270174\n",
            "real input: -5.35819054 real std: 0.35524052381515503fake input: -5.67510128  random input: -5.40299416 line input: -5.58065844\n",
            "real input: -5.68937063 real std: 0.40280061960220337fake input: -6.04726315  random input: -5.92459202 line input: -6.11723900\n",
            "real input: -6.43872023 real std: 0.26775163412094116fake input: -6.64390087  random input: -6.54059601 line input: -6.60895300\n",
            "real input: -7.13423824 real std: 0.41440290212631226fake input: -7.41802454  random input: -7.18240643 line input: -7.34267807\n",
            "real input: -7.90645361 real std: 0.2927164137363434fake input: -8.12379646  random input: -8.03710365 line input: -8.05986214\n",
            "real input: -6.73321247 real std: 0.2984859347343445fake input: -6.99031353  random input: -6.84888554 line input: -6.92215300\n",
            "real input: -9.49843216 real std: 0.28527724742889404fake input: -9.74486160  random input: -9.62295341 line input: -9.68355465\n",
            "real input: -9.91893864 real std: 0.34392109513282776fake input: -10.20790768  random input: -10.13671398 line input: -10.17617226\n",
            "real input: -10.29322338 real std: 0.31411704421043396fake input: -10.49416924  random input: -10.41528130 line input: -10.35667706\n",
            "real input: -10.82217789 real std: 0.3456353545188904fake input: -11.07267761  random input: -10.93684483 line input: -10.91749668\n",
            "real input: -10.54402161 real std: 0.4086564779281616fake input: -10.73602581  random input: -10.66297245 line input: -10.63591290\n",
            "real input: -14.35567856 real std: 0.2667834162712097fake input: -14.54981041  random input: -14.48449516 line input: -14.44420147\n",
            "real input: -14.90801907 real std: 0.31178218126296997fake input: -15.08386135  random input: -15.05452824 line input: -15.04867554\n",
            "real input: -13.14026928 real std: 0.2997049391269684fake input: -13.37533951  random input: -13.30827141 line input: -13.26640892\n",
            "real input: -14.28787041 real std: 0.2540941536426544fake input: -14.50667667  random input: -14.41556072 line input: -14.38220978\n",
            "real input: -12.07055855 real std: 0.30804431438446045fake input: -12.39480019  random input: -12.23530960 line input: -12.18404102\n",
            "real input: -10.08187389 real std: 0.35593166947364807fake input: -10.35986137  random input: -10.35699368 line input: -10.20833874\n",
            "real input: -12.04012585 real std: 0.3533211946487427fake input: -12.25581074  random input: -12.20427704 line input: -12.08746052\n",
            "real input: -13.67735863 real std: 0.26615220308303833fake input: -13.88764858  random input: -13.87853336 line input: -13.67403507\n",
            "real input: -8.89504242 real std: 0.27127841114997864fake input: -9.16190052  random input: -9.11508656 line input: -8.95492458\n",
            "real input: -12.63583088 real std: 0.2881886959075928fake input: -12.85158348  random input: -12.85538101 line input: -12.69039822\n",
            "real input: -17.52976036 real std: 0.2664780020713806fake input: -17.65291977  random input: -17.68441963 line input: -17.50089073\n",
            "real input: -8.44881535 real std: 0.3383294939994812fake input: -8.71730614  random input: -8.69148159 line input: -8.59084606\n",
            "real input: -9.76432323 real std: 0.3241899609565735fake input: -9.97052097  random input: -10.03828335 line input: -9.87271881\n",
            "real input: -7.23380947 real std: 0.3313309848308563fake input: -7.45071220  random input: -7.44289303 line input: -7.31352091\n",
            "real input: -6.41069698 real std: 0.3521576523780823fake input: -6.55597687  random input: -6.68421364 line input: -6.56122065\n",
            "real input: -8.09851551 real std: 0.2814604938030243fake input: -8.31382656  random input: -8.34212589 line input: -8.19004631\n",
            "real input: -8.69439125 real std: 0.24477969110012054fake input: -8.88074684  random input: -8.94149494 line input: -8.78156948\n",
            "real input: -13.45378304 real std: 0.24588440358638763fake input: -13.58045101  random input: -13.73700523 line input: -13.52847862\n",
            "real input: -11.34122849 real std: 0.3892875909805298fake input: -11.52804184  random input: -11.67117691 line input: -11.47396374\n",
            "real input: -11.91423035 real std: 0.31770259141921997fake input: -12.14762688  random input: -12.23687077 line input: -12.05149746\n",
            "real input: -10.73412704 real std: 0.2667388319969177fake input: -10.89340401  random input: -11.06128120 line input: -10.85735035\n",
            "real input: -11.25194359 real std: 0.3130897581577301fake input: -11.52445889  random input: -11.61542988 line input: -11.40452862\n",
            "real input: -11.29018593 real std: 0.27338868379592896fake input: -11.54222775  random input: -11.75207806 line input: -11.52939606\n",
            "real input: -9.27443600 real std: 0.2931811511516571fake input: -9.44767666  random input: -9.68064022 line input: -9.46745205\n",
            "real input: -11.60900974 real std: 0.3181166648864746fake input: -11.78049660  random input: -12.08299637 line input: -11.73454666\n",
            "real input: -9.54826546 real std: 0.3319873511791229fake input: -9.65908337  random input: -10.02933598 line input: -9.75504494\n",
            "real input: -13.29190350 real std: 0.2098456621170044fake input: -13.42898846  random input: -13.69153404 line input: -13.37245464\n",
            "real input: -10.25953197 real std: 0.31152981519699097fake input: -10.43842220  random input: -10.75053596 line input: -10.45550823\n",
            "real input: -7.02564144 real std: 0.2720571458339691fake input: -7.22330379  random input: -7.53645992 line input: -7.23325205\n",
            "real input: -8.44805813 real std: 0.26777905225753784fake input: -8.66062737  random input: -8.90913105 line input: -8.60037613\n",
            "real input: -10.71098995 real std: 0.25969746708869934fake input: -10.90193367  random input: -11.17829418 line input: -10.88018990\n",
            "real input: -11.65363121 real std: 0.2293774038553238fake input: -11.86660767  random input: -12.12002373 line input: -11.79408932\n",
            "real input: -9.85089016 real std: 0.37053292989730835fake input: -10.07289410  random input: -10.43244934 line input: -10.02903652\n",
            "real input: -15.81950760 real std: 0.21659894287586212fake input: -15.92918205  random input: -16.32390785 line input: -15.80897045\n",
            "real input: -14.82371902 real std: 0.2460329830646515fake input: -14.97123432  random input: -15.27431297 line input: -14.88766289\n",
            "real input: -12.05746269 real std: 0.2386101484298706fake input: -12.27062321  random input: -12.61285114 line input: -12.22105503\n",
            "real input: -10.33801079 real std: 0.30152809619903564fake input: -10.45107746  random input: -10.89328575 line input: -10.49331474\n",
            "real input: -10.94761467 real std: 0.30744630098342896fake input: -11.11770725  random input: -11.52997589 line input: -11.16696262\n",
            "real input: -8.96610546 real std: 0.21959194540977478fake input: -9.19427586  random input: -9.54123878 line input: -9.18984604\n",
            "real input: -9.58385658 real std: 0.21223919093608856fake input: -9.82740593  random input: -10.14025974 line input: -9.72551537\n",
            "real input: -10.77276707 real std: 0.32320570945739746fake input: -10.91998863  random input: -11.38453102 line input: -10.98786736\n",
            "real input: -9.58239079 real std: 0.25469085574150085fake input: -9.77771950  random input: -10.13934612 line input: -9.79549122\n",
            "real input: -10.56947899 real std: 0.23873019218444824fake input: -10.73778629  random input: -11.05437660 line input: -10.67636108\n",
            "real input: -12.64743423 real std: 0.2587798833847046fake input: -12.82054329  random input: -13.10875988 line input: -12.76107502\n",
            "real input: -14.63472176 real std: 0.21040557324886322fake input: -14.81526661  random input: -15.10149193 line input: -14.79625702\n",
            "real input: -16.78551674 real std: 0.19010265171527863fake input: -16.91133499  random input: -17.25789642 line input: -16.87161636\n",
            "real input: -15.98663712 real std: 0.2798381745815277fake input: -16.15036392  random input: -16.56683159 line input: -16.14120293\n",
            "real input: -15.44844055 real std: 0.21798175573349fake input: -15.51360989  random input: -15.95884418 line input: -15.56268024\n",
            "real input: -13.02133179 real std: 0.25428536534309387fake input: -13.19211769  random input: -13.49885273 line input: -13.22853279\n",
            "real input: -16.50520134 real std: 0.21850022673606873fake input: -16.65069389  random input: -16.97620392 line input: -16.66669083\n",
            "real input: -16.18576050 real std: 0.22430294752120972fake input: -16.33064842  random input: -16.68631363 line input: -16.30951691\n",
            "real input: -16.46902657 real std: 0.20616351068019867fake input: -16.57063103  random input: -16.97523117 line input: -16.56805038\n",
            "real input: -15.55460548 real std: 0.1996406763792038fake input: -15.67244530  random input: -16.11296082 line input: -15.64210224\n",
            "real input: -16.00289345 real std: 0.2889542281627655fake input: -16.17928886  random input: -16.61022186 line input: -16.18768501\n",
            "real input: -22.58108711 real std: 0.19897283613681793fake input: -22.65399742  random input: -23.06246567 line input: -22.69819832\n",
            "real input: -20.56364059 real std: 0.22980143129825592fake input: -20.68359947  random input: -21.04686546 line input: -20.64625168\n",
            "real input: -26.22252846 real std: 0.18473286926746368fake input: -26.28154182  random input: -26.68837166 line input: -26.30194473\n",
            "real input: -16.97068787 real std: 0.24219422042369843fake input: -17.06056404  random input: -17.47237015 line input: -17.14280128\n",
            "real input: -15.50509357 real std: 0.18308986723423004fake input: -15.64907646  random input: -16.01065636 line input: -15.70453739\n",
            "real input: -13.61182594 real std: 0.21814504265785217fake input: -13.73798084  random input: -14.10890388 line input: -13.76937675\n",
            "real input: -12.45062542 real std: 0.23466582596302032fake input: -12.54152679  random input: -12.98696709 line input: -12.61567593\n",
            "real input: -12.33694553 real std: 0.19313013553619385fake input: -12.47853756  random input: -12.87104511 line input: -12.48375893\n",
            "real input: -12.03503513 real std: 0.21287696063518524fake input: -12.10544014  random input: -12.55505276 line input: -12.23895454\n",
            "real input: -10.78169727 real std: 0.19023241102695465fake input: -10.93293476  random input: -11.31771851 line input: -10.98628330\n",
            "real input: -12.68599796 real std: 0.23323047161102295fake input: -12.79113483  random input: -13.23375416 line input: -12.90212631\n",
            "real input: -14.54304409 real std: 0.18109680712223053fake input: -14.69900131  random input: -15.03348351 line input: -14.66655350\n",
            "real input: -16.03833961 real std: 0.2083260715007782fake input: -16.15096855  random input: -16.44261169 line input: -16.12161255\n",
            "real input: -14.79308701 real std: 0.2578583359718323fake input: -14.87191200  random input: -15.26026917 line input: -14.91834641\n",
            "real input: -12.64450073 real std: 0.18186834454536438fake input: -12.74248314  random input: -13.10712242 line input: -12.74388504\n",
            "real input: -14.49039650 real std: 0.25571754574775696fake input: -14.59473133  random input: -14.93552303 line input: -14.65896511\n",
            "real input: -15.17959976 real std: 0.20081736147403717fake input: -15.28249454  random input: -15.63115311 line input: -15.26225376\n",
            "real input: -12.95464993 real std: 0.1820165365934372fake input: -13.09801674  random input: -13.39765358 line input: -13.15746307\n",
            "real input: -8.79345322 real std: 0.162163645029068fake input: -8.91984367  random input: -9.18373394 line input: -9.00517654\n",
            "real input: -9.34467793 real std: 0.18674024939537048fake input: -9.42696667  random input: -9.78803539 line input: -9.55816841\n",
            "real input: -9.67383671 real std: 0.22323471307754517fake input: -9.77975082  random input: -10.16765785 line input: -9.82770538\n",
            "real input: -8.19648457 real std: 0.26933664083480835fake input: -8.22102070  random input: -8.63548279 line input: -8.37476063\n",
            "real input: -10.04541111 real std: 0.22438327968120575fake input: -10.13273144  random input: -10.49104309 line input: -10.20298481\n",
            "real input: -11.30962658 real std: 0.22313576936721802fake input: -11.40713882  random input: -11.75395679 line input: -11.50646877\n",
            "real input: -9.87023640 real std: 0.18914411962032318fake input: -9.99055958  random input: -10.28295517 line input: -10.06351566\n",
            "real input: -12.04356575 real std: 0.1884378343820572fake input: -12.15582371  random input: -12.46948051 line input: -12.24822140\n",
            "real input: -13.92134094 real std: 0.18207183480262756fake input: -14.04562855  random input: -14.34790039 line input: -14.10281658\n",
            "real input: -14.79728985 real std: 0.1759510487318039fake input: -14.84003544  random input: -15.17506695 line input: -14.92911720\n",
            "real input: -10.84897327 real std: 0.2702731490135193fake input: -11.00664043  random input: -11.31369305 line input: -11.11517525\n",
            "real input: -11.75442123 real std: 0.1715129315853119fake input: -11.85537624  random input: -12.18594551 line input: -11.94166565\n",
            "real input: -11.23524570 real std: 0.17000143229961395fake input: -11.34308720  random input: -11.69233608 line input: -11.44197083\n",
            "real input: -9.92885017 real std: 0.19943402707576752fake input: -10.03530121  random input: -10.40301323 line input: -10.21497059\n",
            "real input: -9.06257820 real std: 0.19207504391670227fake input: -9.18747616  random input: -9.59075737 line input: -9.34350967\n",
            "real input: -10.55495262 real std: 0.19006489217281342fake input: -10.69467831  random input: -11.06752968 line input: -10.86016750\n",
            "real input: -10.01663685 real std: 0.16361427307128906fake input: -10.15650940  random input: -10.50987530 line input: -10.22034168\n",
            "real input: -12.19211483 real std: 0.15549293160438538fake input: -12.31851482  random input: -12.71955395 line input: -12.44303513\n",
            "real input: -10.74053192 real std: 0.1466706246137619fake input: -10.86200237  random input: -11.20666599 line input: -11.01094341\n",
            "real input: -12.08285904 real std: 0.20536069571971893fake input: -12.21455288  random input: -12.54538250 line input: -12.23988724\n",
            "real input: -10.97119617 real std: 0.27744829654693604fake input: -11.02963257  random input: -11.33995247 line input: -11.09306622\n",
            "real input: -11.71490669 real std: 0.2052416354417801fake input: -11.84680557  random input: -12.15162373 line input: -11.93451500\n",
            "real input: -11.31487179 real std: 0.191860169172287fake input: -11.38014317  random input: -11.81628609 line input: -11.59451771\n",
            "real input: -12.88668346 real std: 0.14684945344924927fake input: -12.97560215  random input: -13.29856300 line input: -13.07478237\n",
            "real input: -10.29229450 real std: 0.12312876433134079fake input: -10.39465141  random input: -10.68380260 line input: -10.51433659\n",
            "real input: -9.64107704 real std: 0.18328091502189636fake input: -9.74345684  random input: -10.01980400 line input: -9.84490681\n",
            "real input: -9.66710281 real std: 0.2035168558359146fake input: -9.79109955  random input: -10.13440990 line input: -9.93955231\n",
            "real input: -9.16704369 real std: 0.18539240956306458fake input: -9.28217888  random input: -9.57487869 line input: -9.39379501\n",
            "real input: -8.65620518 real std: 0.17756019532680511fake input: -8.76253033  random input: -9.05057049 line input: -8.86694717\n",
            "real input: -9.92802811 real std: 0.1846688985824585fake input: -10.06186390  random input: -10.38659573 line input: -10.17682743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB4gy8hQ79Nl"
      },
      "source": [
        "saveModel('/content/', 500)\r\n",
        "wandb.save('epoch500.tar', base_path='/content')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biQxVa_O26p7"
      },
      "source": [
        "num_samples = 9\r\n",
        "num_classes = 40\r\n",
        "df = pd.DataFrame(index=range(num_samples), columns=[str(x) for x in range(num_classes)])\r\n",
        "for i in range(num_samples):\r\n",
        "    df.iloc[i] = torch.randn(1, num_classes).numpy().round(2)\r\n",
        "\r\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHkJHeAOQrii"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpF5kN1O7d1b",
        "scrolled": false
      },
      "source": [
        "import os\n",
        "\n",
        "torch.save({\n",
        "        'generator_state_dict': generator.state_dict(),\n",
        "        'discriminator_state_dict': discriminator.state_dict(),\n",
        "        'optimizerG_state_dict': generator_opt.state_dict(),\n",
        "        'optimizerD_state_dict': discriminator_opt.state_dict()   \n",
        "        }, 'wgan3.tar')\n",
        "print('Model saved!')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6srp5M5e8csD"
      },
      "source": [
        "testNoise = noise(sample_size, int(seq_length*1), noise_features)\n",
        "testOut = generator(testNoise)\n",
        "for n in testOut:\n",
        "  plt.plot(n.detach().cpu())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhIEroKk7d1e",
        "scrolled": false
      },
      "source": [
        "#from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "generator.eval()\n",
        "discriminator.eval()\n",
        "\n",
        "testNoise = noise(2, int(seq_length*1), noise_features)\n",
        "testOut = generator(testNoise)\n",
        "\n",
        "for n in testOut:\n",
        "    plt.plot(n.detach().cpu())\n",
        "plt.show()\n",
        "\n",
        "# plt.show()\n",
        "# plt.figure(figsize=(16,10))\n",
        "# plt.plot(testOut[0].detach().cpu(), 'black') \n",
        "# plt.plot(testOut[0].detach().cpu() , 'bisque')\n",
        "# plt.plot(testOut[1].detach().cpu() , 'aquamarine')\n",
        "# plt.plot(testOut[2].detach().cpu() , 'lightsteelblue')\n",
        "# plt.plot(testOut[3].detach().cpu() , 'lightcoral')\n",
        "# plt.plot(testOut[4].detach().cpu() , 'cornsilk')\n",
        "# plt.plot(testOut[5].detach().cpu() , 'thistle')\n",
        "# plt.plot(testOut[6].detach().cpu() , 'peachpuff')\n",
        "# plt.plot(testOut[7].detach().cpu() , 'powderblue')\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "r1u =  0 # delta\n",
        "r1l = -0 # delta\n",
        "r2u =  0 # amplitude\n",
        "r2l = -0 # amplitude\n",
        "r3u =  0 # freq (+ amplitude)\n",
        "r3l = -0 # freq (+ amplitude)\n",
        "r4u =  0 # freq (+ amplitude)\n",
        "r4l = -0 # freq (+ amplitude)\n",
        "\n",
        "for b in range(2):\n",
        "  r1 = random.uniform(r1u,r1l)\n",
        "  r2 = random.uniform(r2u,r2l)\n",
        "  r3 = random.uniform(r3u,r3l)    #-> noise_features = 3\n",
        "  r4 = random.uniform(r4u,r4l)\n",
        "  #r4 = random.uniform(-1,1)   #-> noise_features = 4 ...\n",
        "  for i in range(seq_length):\n",
        "    testNoise[b][i][0] = r1  \n",
        "    testNoise[b][i][1] = r2 \n",
        "    testNoise[b][i][2] = r3 \n",
        "  #  testNoise[b][i][3] = r4\n",
        "    #noise[b][i][3] = r4\n",
        "    \n",
        "# fig = plt.figure(constrained_layout=True)\n",
        "# ax = fig.add_subplot( projection='3d')  \n",
        "# for b in range(sample_size):\n",
        "#     ax.scatter(testNoise[b][0][0].cpu(), testNoise[b][0][1].cpu(), testNoise[b][0][2].cpu(), marker=\".\")\n",
        "\n",
        "# ax.set_xlabel('Freq')\n",
        "# ax.set_ylabel('Ampl')\n",
        "# ax.set_zlabel('delta')\n",
        "# plt.show()\n",
        "for i in range(seq_length):\n",
        "    testNoise[0][i][0] = r1  \n",
        "    testNoise[0][i][1] = r2 \n",
        "    testNoise[0][i][2] = r3 \n",
        "#    testNoise[0][i][3] = r4\n",
        "    testNoise[1][i][0] = r1 + 0.\n",
        "    testNoise[1][i][1] = r2 + 0.\n",
        "    testNoise[1][i][2] = r3 + 0.9  # Yoffset\n",
        "#    testNoise[0][i][3] = r4 + -0.\n",
        "testOut = generator(testNoise)\n",
        "\n",
        "plt.show()\n",
        "plt.figure(figsize=(16,10))\n",
        "plt.plot(testOut[0].detach().cpu(), 'black') \n",
        "plt.plot(testOut[1].detach().cpu() , 'red')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeNa791VfEVU"
      },
      "source": [
        "# checkpoint = torch.load('/content/epoch9999.tar')#torch.load(loadPath)\r\n",
        "# generator.load_state_dict(checkpoint['generator_state_dict'])\r\n",
        "# discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\r\n",
        "# generator_opt.load_state_dict(checkpoint['optimizerG_state_dict'])\r\n",
        "# discriminator_opt.load_state_dict(checkpoint['optimizerD_state_dict'])\r\n",
        "# print('Models loaded!')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyypbYhzRIbG"
      },
      "source": [
        "#rnoise = torch.ones( (4, seq_length, noise_features)).to(device)\r\n",
        "avg_size = 1000\r\n",
        "\r\n",
        "rnoise = torch.FloatTensor(avg_size, seq_length).uniform_(-1, 1)\r\n",
        "# for i, s in enumerate(rnoise): \r\n",
        "#   rnoise[i] = s * random.uniform(-2, 2)\r\n",
        "rnoise_out = discriminator(rnoise)\r\n",
        "\r\n",
        "line = torch.zeros(avg_size, seq_length) \r\n",
        "for i, s in enumerate(line): \r\n",
        "  line[i] = s + random.uniform(-1, 1)\r\n",
        "\r\n",
        "line_out = discriminator(line)\r\n",
        "\r\n",
        "#print(f'random input: {rnoise_out}')\r\n",
        "#print(f'line input: {line_out}')\r\n",
        "def sineWave1(size = 1, rand=1, amp=0, rad=0, phase=0, offsetY=0):\r\n",
        "  res = torch.zeros(size, 50)\r\n",
        "  for t in range(size):\r\n",
        "    #a = np.arange(0.1,0.9,0.02)\r\n",
        "    x = np.arange(0,20,0.4)\r\n",
        "    #r = np.arange(2,6.1,0.1)\r\n",
        "    count = 0\r\n",
        "    fs = len(x)\r\n",
        "    #y = np.zeros((1,len(x)))\r\n",
        "    #y = [50]\r\n",
        "    if rand:\r\n",
        "      amp = random.uniform(0.1*1, 0.9*1)\r\n",
        "      rad = random.uniform(5, 10)\r\n",
        "      phase = random.uniform(-fs/0.5*np.pi,fs/0.5*np.pi)\r\n",
        "      offsetY =  0# random.uniform(-1, 1) *1\r\n",
        "  \r\n",
        "    y = offsetY + amp*np.sin(((2*np.pi*rad*x)+phase)/fs) \r\n",
        "    if 0:\r\n",
        "      for i, s in enumerate(y): \r\n",
        "        y[i] = s + random.uniform(-0.05, 0.001*i)\r\n",
        "    y = torch.from_numpy(y).to(dtype=torch.float)\r\n",
        "    res[t] = y\r\n",
        "\r\n",
        "  return res.unsqueeze(2)\r\n",
        "\r\n",
        "sine = sineWave1(avg_size)\r\n",
        "\r\n",
        "fake = generator(noise(avg_size, seq_length, noise_features))\r\n",
        "fake_out = discriminator(fake)\r\n",
        "#print(f'sine: {sine.shape}')\r\n",
        "# plt.plot(sine[0])\r\n",
        "# plt.plot(sine[1])\r\n",
        "# plt.plot(rnoise[0])\r\n",
        "# plt.plot(line[0])\r\n",
        "# plt.show()\r\n",
        "#sine_tensor = torch.from_numpy(sine).unsqueeze(0).unsqueeze(2).to(device=device, dtype=torch.float)\r\n",
        "\r\n",
        "real_out = discriminator(sine)\r\n",
        "#print(f'real input: {real_out}')\r\n",
        "print(f'real input: {real_out.mean():.8f} real std: {real_out.std() }fake input: {fake_out.mean():.8f}  random input: {rnoise_out.mean():.8f} line input: {line_out.mean():.8f}')\r\n",
        "\r\n",
        "data = [real_out.detach().squeeze(1).numpy(), rnoise_out.detach().squeeze(1).numpy(), line_out.detach().squeeze(1).numpy(), fake_out.detach().squeeze(1).numpy()]\r\n",
        "# fig, ax = plt.subplots()\r\n",
        "# fig.set_size_inches(18.5, 10.5)\r\n",
        "# bp1 = ax.boxplot(data[0], positions=[1], notch=True, widths=0.35, \r\n",
        "#                  patch_artist=True, boxprops=dict(facecolor=\"C0\"))\r\n",
        "# bp2 = ax.boxplot(data[1], positions=[2], notch=True, widths=0.35, \r\n",
        "#                  patch_artist=True, boxprops=dict(facecolor=\"C2\"))\r\n",
        "# bp3 = ax.boxplot(data[2], positions=[3], notch=True, widths=0.35, \r\n",
        "#                  patch_artist=True, boxprops=dict(facecolor=\"C3\"))\r\n",
        "# ax.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0], bp3[\"boxes\"][0]], ['real', 'noise uniform(-1, 1)', 'line uniform(-1, 1)'])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "hist_data = [data[0], data[1], data[2], data[3]]\r\n",
        "\r\n",
        "group_labels = ['real', 'noise', 'line', 'generator']\r\n",
        "\r\n",
        "# Create distplot with custom bin_size\r\n",
        "fig = ff.create_distplot(hist_data, group_labels, bin_size=math.pow(0.5/ math.log(avg_size),2))\r\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhBI8hjSiN-z"
      },
      "source": [
        "#wandb.log({'RealvsNoise ' + str(avg_size): fig})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sanl8FgD9eS8"
      },
      "source": [
        "\r\n",
        "\r\n",
        "#data = [real_out.detach().squeeze(1).numpy(), rnoise_out.detach().squeeze(1).numpy(), line_out.detach().squeeze(1).numpy(), fake_out.detach().squeeze(1).numpy()]\r\n",
        "fig, ax = plt.subplots()\r\n",
        "fig.set_size_inches(18.5, 10.5)\r\n",
        "bp1 = ax.boxplot(data[0], positions=[1], notch=True, widths=0.35, \r\n",
        "                 patch_artist=True, boxprops=dict(facecolor=\"C0\"))\r\n",
        "bp2 = ax.boxplot(data[1], positions=[2], notch=True, widths=0.35, \r\n",
        "                 patch_artist=True, boxprops=dict(facecolor=\"C2\"))\r\n",
        "bp3 = ax.boxplot(data[2], positions=[3], notch=True, widths=0.35, \r\n",
        "                 patch_artist=True, boxprops=dict(facecolor=\"C3\"))\r\n",
        "ax.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0], bp3[\"boxes\"][0]], ['real', 'noise uniform(-1, 1)', 'line uniform(-1, 1)'])\r\n",
        "# ax.annotate('outlier', xy=(2, 1), xytext=(1.5, 1),\r\n",
        "#             arrowprops=dict(facecolor='black', shrink=0.05))\r\n",
        "\r\n",
        "# ax.text(1.5, 0.8, 'Boxplot of critic output over 10.000 samples each', style='italic',\r\n",
        "#         bbox={'facecolor': 'red', 'alpha': 0.5, 'pad': 10})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bOyRyposElb"
      },
      "source": [
        "#data = [real_out.detach().squeeze(1).numpy(), rnoise_out.detach().squeeze(1).numpy(), line_out.detach().squeeze(1).numpy(), fake_out.detach().squeeze(1).numpy()]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# Group data together\r\n",
        "hist_data = [data[0], data[1], data[2], data[3]]\r\n",
        "\r\n",
        "group_labels = ['real', 'noise', 'line', 'generator']\r\n",
        "\r\n",
        "# Create distplot with custom bin_size\r\n",
        "fig = ff.create_distplot(hist_data, group_labels, bin_size=[100/avg_size, 100/avg_size, 100/avg_size, 100/avg_size])\r\n",
        "fig.show()\r\n",
        "#wandb.log({'RealvsNoise ' + str(epoch): fig})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDKRhhzcd6-K"
      },
      "source": [
        "diffSine = torch.zeros(500, 50, 1)\r\n",
        "\r\n",
        "for i in range(500):\r\n",
        "  diffSine[i] = sineWave1(size=1, rand= 1, amp=0.55, rad=5, phase=2, offsetY=0) \r\n",
        "  plt.plot(diffSine[i])\r\n",
        "plt.show()\r\n",
        "\r\n",
        "\r\n",
        "diffSine_out =  discriminator(diffSine)\r\n",
        "print('-2.45 bis -2.9')\r\n",
        "print(diffSine_out)\r\n",
        "\r\n",
        "for i, s in enumerate(diffSine_out.squeeze(0)): \r\n",
        "  #line[i] = s + random.uniform(0.3, 1)\r\n",
        " if diffSine_out[i] < -2.9:\r\n",
        "    plt.plot(diffSine[i])\r\n",
        "    print(f'dangerous: {i}')\r\n",
        "\r\n",
        "  # if diffSine_out[i] > -2.45:\r\n",
        "  # #  if real_out[i] > -3.35:\r\n",
        "  #     plt.plot(diffSine[i])\r\n",
        "  #     print(f'maybe ok: {i}')\r\n",
        " else:\r\n",
        "    print(f'good : {i}')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z253rvMxvGUe"
      },
      "source": [
        "for i, s in enumerate(real_out.squeeze(0)): \r\n",
        "  #line[i] = s + random.uniform(0.3, 1)\r\n",
        "  if real_out[i] > -3.65 and real_out[i] < -3.645:\r\n",
        "#  if real_out[i] > -3.35:\r\n",
        "    plt.plot(sine[i])\r\n",
        "\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u25BUsetwCfC"
      },
      "source": [
        "# Averaged over batchsize of 100.000\r\n",
        "# base-line real: ~3.12           stdev: 0.69\r\n",
        "# random: ~3.89\r\n",
        "# 2-line: ~4.89\r\n",
        "# 1-line: ~4.18\r\n",
        "\r\n",
        "# higher freq: 3.26\r\n",
        "# amp * 10: 3.58\r\n",
        "# amp * 2: 3.06\r\n",
        "# offsetY * 10: 3.25\r\n",
        "# offsetY * 2: 2.99\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}